WARNING: eval_every must be divisible by batch_size
Setting eval_every to 19968
no FSDP port specified; using open port for FSDP: 42679
seed: 0
exp_name: qwen3-4b-test-1-sft
batch_size: 128
eval_batch_size: 8
debug: false
fsdp_port: 42679
datasets:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/dataset/ultra-train-dataset.jsonl
wandb:
  enabled: true
  entity: null
  project: direct-preference-optimization
local_dirs:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-4b-test-1-sft
lr: 5.0e-06
gradient_accumulation_steps: 8
max_grad_norm: 10.0
max_length: 2048
max_prompt_length: 512
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 19968
minimum_log_interval_secs: 1.0
model:
  name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-4B
  tokenizer_name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-4B
  archive: null
  block_name: Qwen3DecoderLayer
  reference_name_or_path: null
  policy_dtype: float32
  fsdp_policy_mp: bfloat16
  reference_dtype: float16
loss:
  name: sft

================================================================================
Writing to t-20250803003347-ql84n-worker-0:/fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-4b-test-1-sft
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.12s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.15s/it]
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 1048576 from 1048576
[rank1]:[W802 16:40:54.202645125 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W802 16:40:59.607104896 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W802 16:41:03.699072464 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W802 16:41:07.856665036 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W802 16:41:11.242290519 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W802 16:41:16.502122969 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W802 16:41:20.640706252 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W802 16:41:20.640791786 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-4B
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 696.8 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 6734.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 6734.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 6734.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 6734.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 6734.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 24% | 内存使用 3642.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 1114.2 MB / 81920.0 MB
Loaded train data iterator
Finished generating 256 examples on test split
Loaded 32 eval batches of size 8
Sharding policy with FSDP...
Loaded model on rank 0
Using RMSprop optimizer
=== Running evaluation after 0 train examples ===
Computing eval metrics:   0%|          | 0/32 [00:00<?, ?it/s]Computing eval metrics:   3%|▎         | 1/32 [00:00<00:16,  1.84it/s]Computing eval metrics:   6%|▋         | 2/32 [00:00<00:08,  3.39it/s]Computing eval metrics:  12%|█▎        | 4/32 [00:00<00:04,  6.13it/s]Computing eval metrics:  19%|█▉        | 6/32 [00:01<00:03,  7.83it/s]Computing eval metrics:  25%|██▌       | 8/32 [00:01<00:02,  8.87it/s]Computing eval metrics:  31%|███▏      | 10/32 [00:01<00:02,  9.65it/s]Computing eval metrics:  38%|███▊      | 12/32 [00:01<00:01, 10.05it/s]Computing eval metrics:  44%|████▍     | 14/32 [00:01<00:01,  9.78it/s]Computing eval metrics:  50%|█████     | 16/32 [00:01<00:01, 10.14it/s]Computing eval metrics:  56%|█████▋    | 18/32 [00:02<00:01, 10.29it/s]Computing eval metrics:  62%|██████▎   | 20/32 [00:02<00:01, 10.70it/s]Computing eval metrics:  69%|██████▉   | 22/32 [00:02<00:00, 10.49it/s]Computing eval metrics:  75%|███████▌  | 24/32 [00:02<00:00, 10.05it/s]Computing eval metrics:  81%|████████▏ | 26/32 [00:02<00:00,  9.96it/s]Computing eval metrics:  88%|████████▊ | 28/32 [00:03<00:00,  9.99it/s]Computing eval metrics:  94%|█████████▍| 30/32 [00:03<00:00,  9.69it/s]Computing eval metrics: 100%|██████████| 32/32 [00:03<00:00,  9.91it/s]Computing eval metrics: 100%|██████████| 32/32 [00:03<00:00,  9.04it/s]
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
6 initializing distributed
Creating trainer on process 6 with world size 8
3 initializing distributed
Creating trainer on process 3 with world size 8
1 initializing distributed
Creating trainer on process 1 with world size 8
5 initializing distributed
Creating trainer on process 5 with world size 8
2 initializing distributed
Creating trainer on process 2 with world size 8
7 initializing distributed
Creating trainer on process 7 with world size 8
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2441:2441 [2] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO ncclCommInitRank comm 0x13c00820 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO NVLS multicast support is not available on dev 2
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO comm 0x13c00820 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2441:3513 [2] NCCL INFO ncclCommInitRank comm 0x13c00820 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x321f6cee1b4de3ff - Init COMPLETE
4 initializing distributed
Creating trainer on process 4 with world size 8
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:3048:3048 [6] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO ncclCommInitRank comm 0x172f7770 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId e0000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO NVLS multicast support is not available on dev 6
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO comm 0x172f7770 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:3048:3512 [6] NCCL INFO ncclCommInitRank comm 0x172f7770 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId e0000 commId 0x321f6cee1b4de3ff - Init COMPLETE
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 12098.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 11922.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 12246.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 12402.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 12402.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 12376.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 12324.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 12254.2 MB / 81920.0 MB
Eval after 0 examples: {'logps_eval/chosen': '-419.26', 'loss/eval': '419.26'}
Train stats after 128 examples: {'logps_train/chosen': '-397', 'loss/train': '397', 'examples_per_second': '22.22', 'grad_norm': '3360.6', 'counters/examples': 128, 'counters/updates': 1}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 44580.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 44482.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 44702.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 44858.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 44858.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 44832.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 44780.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 44710.2 MB / 81920.0 MB
Train stats after 256 examples: {'logps_train/chosen': '-441.34', 'loss/train': '441.34', 'examples_per_second': '20.483', 'grad_norm': '3756.5', 'counters/examples': 256, 'counters/updates': 2}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 52794.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 52670.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 52890.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 53046.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 53046.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 53020.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52968.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 52898.2 MB / 81920.0 MB
Train stats after 384 examples: {'logps_train/chosen': '-402.18', 'loss/train': '402.18', 'examples_per_second': '18.008', 'grad_norm': '3518', 'counters/examples': 384, 'counters/updates': 3}
Train stats after 512 examples: {'logps_train/chosen': '-405.8', 'loss/train': '405.8', 'examples_per_second': '19.73', 'grad_norm': '3408.2', 'counters/examples': 512, 'counters/updates': 4}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52846.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 52748.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 52942.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 53046.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 53046.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 53020.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52968.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52898.2 MB / 81920.0 MB
Train stats after 640 examples: {'logps_train/chosen': '-378.77', 'loss/train': '378.77', 'examples_per_second': '21.984', 'grad_norm': '3206.8', 'counters/examples': 640, 'counters/updates': 5}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 61234.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 61136.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 61330.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 61434.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 61434.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 61408.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 61356.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 61286.2 MB / 81920.0 MB
Train stats after 768 examples: {'logps_train/chosen': '-344.11', 'loss/train': '344.11', 'examples_per_second': '17.938', 'grad_norm': '3080.7', 'counters/examples': 768, 'counters/updates': 6}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 61234.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 61136.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 61330.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 61434.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 61434.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 61408.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 61356.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 61286.2 MB / 81920.0 MB
Train stats after 896 examples: {'logps_train/chosen': '-406.88', 'loss/train': '406.88', 'examples_per_second': '18.392', 'grad_norm': '3279.9', 'counters/examples': 896, 'counters/updates': 7}
Train stats after 1024 examples: {'logps_train/chosen': '-378.12', 'loss/train': '378.12', 'examples_per_second': '21.73', 'grad_norm': '2940.2', 'counters/examples': 1024, 'counters/updates': 8}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68158.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 68060.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 68254.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 68332.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 96% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 68210.2 MB / 81920.0 MB
Train stats after 1152 examples: {'logps_train/chosen': '-352.75', 'loss/train': '352.75', 'examples_per_second': '16.349', 'grad_norm': '2762.4', 'counters/examples': 1152, 'counters/updates': 9}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68254.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 1280 examples: {'logps_train/chosen': '-379.94', 'loss/train': '379.94', 'examples_per_second': '25.605', 'grad_norm': '2889.4', 'counters/examples': 1280, 'counters/updates': 10}
Train stats after 1408 examples: {'logps_train/chosen': '-404.75', 'loss/train': '404.75', 'examples_per_second': '22.88', 'grad_norm': '2821.4', 'counters/examples': 1408, 'counters/updates': 11}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68254.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 1536 examples: {'logps_train/chosen': '-395.95', 'loss/train': '395.95', 'examples_per_second': '16.673', 'grad_norm': '2361.8', 'counters/examples': 1536, 'counters/updates': 12}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 1664 examples: {'logps_train/chosen': '-370.73', 'loss/train': '370.73', 'examples_per_second': '23.377', 'grad_norm': '2230.4', 'counters/examples': 1664, 'counters/updates': 13}
Train stats after 1792 examples: {'logps_train/chosen': '-393.21', 'loss/train': '393.21', 'examples_per_second': '18.634', 'grad_norm': '2294.1', 'counters/examples': 1792, 'counters/updates': 14}
[GPU Monitor] GPU 0: 利用率 42% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 26% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 11% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 38% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 1920 examples: {'logps_train/chosen': '-375.16', 'loss/train': '375.16', 'examples_per_second': '18.071', 'grad_norm': '2057.2', 'counters/examples': 1920, 'counters/updates': 15}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2048 examples: {'logps_train/chosen': '-340.79', 'loss/train': '340.79', 'examples_per_second': '21.989', 'grad_norm': '1760.7', 'counters/examples': 2048, 'counters/updates': 16}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2176 examples: {'logps_train/chosen': '-399.06', 'loss/train': '399.06', 'examples_per_second': '20.912', 'grad_norm': '1669.6', 'counters/examples': 2176, 'counters/updates': 17}
Train stats after 2304 examples: {'logps_train/chosen': '-395.66', 'loss/train': '395.66', 'examples_per_second': '20.527', 'grad_norm': '1246.1', 'counters/examples': 2304, 'counters/updates': 18}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2432 examples: {'logps_train/chosen': '-340.72', 'loss/train': '340.72', 'examples_per_second': '22.911', 'grad_norm': '942.22', 'counters/examples': 2432, 'counters/updates': 19}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 95% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 95% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2560 examples: {'logps_train/chosen': '-339.04', 'loss/train': '339.04', 'examples_per_second': '17.8', 'grad_norm': '987.71', 'counters/examples': 2560, 'counters/updates': 20}
Train stats after 2688 examples: {'logps_train/chosen': '-298.18', 'loss/train': '298.18', 'examples_per_second': '21.61', 'grad_norm': '731.76', 'counters/examples': 2688, 'counters/updates': 21}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2816 examples: {'logps_train/chosen': '-331.99', 'loss/train': '331.99', 'examples_per_second': '17.451', 'grad_norm': '675.01', 'counters/examples': 2816, 'counters/updates': 22}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 2944 examples: {'logps_train/chosen': '-386.52', 'loss/train': '386.52', 'examples_per_second': '21.394', 'grad_norm': '796.79', 'counters/examples': 2944, 'counters/updates': 23}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68164.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68280.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68358.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
Train stats after 3072 examples: {'logps_train/chosen': '-336.6', 'loss/train': '336.6', 'examples_per_second': '16.466', 'grad_norm': '618.05', 'counters/examples': 3072, 'counters/updates': 24}
Train stats after 3200 examples: {'logps_train/chosen': '-269.82', 'loss/train': '269.82', 'examples_per_second': '21.639', 'grad_norm': '742.27', 'counters/examples': 3200, 'counters/updates': 25}
[GPU Monitor] GPU 0: 利用率 83% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 3328 examples: {'logps_train/chosen': '-311.22', 'loss/train': '311.22', 'examples_per_second': '24.669', 'grad_norm': '514.42', 'counters/examples': 3328, 'counters/updates': 26}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 3456 examples: {'logps_train/chosen': '-330.54', 'loss/train': '330.54', 'examples_per_second': '21.368', 'grad_norm': '534.82', 'counters/examples': 3456, 'counters/updates': 27}
Train stats after 3584 examples: {'logps_train/chosen': '-327.13', 'loss/train': '327.13', 'examples_per_second': '20.277', 'grad_norm': '732.25', 'counters/examples': 3584, 'counters/updates': 28}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 3712 examples: {'logps_train/chosen': '-320.28', 'loss/train': '320.28', 'examples_per_second': '21.53', 'grad_norm': '639.76', 'counters/examples': 3712, 'counters/updates': 29}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 3840 examples: {'logps_train/chosen': '-315.98', 'loss/train': '315.98', 'examples_per_second': '17.061', 'grad_norm': '366.86', 'counters/examples': 3840, 'counters/updates': 30}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 3968 examples: {'logps_train/chosen': '-339.88', 'loss/train': '339.88', 'examples_per_second': '19.995', 'grad_norm': '369.9', 'counters/examples': 3968, 'counters/updates': 31}
Train stats after 4096 examples: {'logps_train/chosen': '-280.93', 'loss/train': '280.93', 'examples_per_second': '22.27', 'grad_norm': '319.41', 'counters/examples': 4096, 'counters/updates': 32}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 4224 examples: {'logps_train/chosen': '-313.03', 'loss/train': '313.03', 'examples_per_second': '23.68', 'grad_norm': '383.71', 'counters/examples': 4224, 'counters/updates': 33}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 4352 examples: {'logps_train/chosen': '-302.23', 'loss/train': '302.23', 'examples_per_second': '21.601', 'grad_norm': '381.29', 'counters/examples': 4352, 'counters/updates': 34}
Train stats after 4480 examples: {'logps_train/chosen': '-309.58', 'loss/train': '309.58', 'examples_per_second': '23.648', 'grad_norm': '463.18', 'counters/examples': 4480, 'counters/updates': 35}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 68262.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 68190.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 68384.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 68288.2 MB / 81920.0 MB
Train stats after 4608 examples: {'logps_train/chosen': '-312.95', 'loss/train': '312.95', 'examples_per_second': '28.667', 'grad_norm': '345.27', 'counters/examples': 4608, 'counters/updates': 36}
Train stats after 4736 examples: {'logps_train/chosen': '-282.84', 'loss/train': '282.84', 'examples_per_second': '20.151', 'grad_norm': '287.48', 'counters/examples': 4736, 'counters/updates': 37}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 95% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 4864 examples: {'logps_train/chosen': '-313.88', 'loss/train': '313.88', 'examples_per_second': '16.14', 'grad_norm': '337.55', 'counters/examples': 4864, 'counters/updates': 38}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 4992 examples: {'logps_train/chosen': '-298.22', 'loss/train': '298.22', 'examples_per_second': '23.415', 'grad_norm': '353.43', 'counters/examples': 4992, 'counters/updates': 39}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 5120 examples: {'logps_train/chosen': '-324.34', 'loss/train': '324.34', 'examples_per_second': '17.56', 'grad_norm': '408.8', 'counters/examples': 5120, 'counters/updates': 40}
Train stats after 5248 examples: {'logps_train/chosen': '-346.25', 'loss/train': '346.25', 'examples_per_second': '22.222', 'grad_norm': '469.4', 'counters/examples': 5248, 'counters/updates': 41}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 5376 examples: {'logps_train/chosen': '-343.42', 'loss/train': '343.42', 'examples_per_second': '22.196', 'grad_norm': '423.34', 'counters/examples': 5376, 'counters/updates': 42}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 5504 examples: {'logps_train/chosen': '-314.79', 'loss/train': '314.79', 'examples_per_second': '21.276', 'grad_norm': '322.93', 'counters/examples': 5504, 'counters/updates': 43}
Train stats after 5632 examples: {'logps_train/chosen': '-272.42', 'loss/train': '272.42', 'examples_per_second': '18.902', 'grad_norm': '275.57', 'counters/examples': 5632, 'counters/updates': 44}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 9% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 5760 examples: {'logps_train/chosen': '-344.09', 'loss/train': '344.09', 'examples_per_second': '22.118', 'grad_norm': '355.2', 'counters/examples': 5760, 'counters/updates': 45}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 5888 examples: {'logps_train/chosen': '-301.96', 'loss/train': '301.96', 'examples_per_second': '22.407', 'grad_norm': '381.78', 'counters/examples': 5888, 'counters/updates': 46}
Train stats after 6016 examples: {'logps_train/chosen': '-277.45', 'loss/train': '277.45', 'examples_per_second': '27.046', 'grad_norm': '383.93', 'counters/examples': 6016, 'counters/updates': 47}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 95% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 6144 examples: {'logps_train/chosen': '-363.99', 'loss/train': '363.99', 'examples_per_second': '19.12', 'grad_norm': '359.78', 'counters/examples': 6144, 'counters/updates': 48}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 77674.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 77602.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 77796.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 77700.2 MB / 81920.0 MB
Train stats after 6272 examples: {'logps_train/chosen': '-274.59', 'loss/train': '274.59', 'examples_per_second': '16.375', 'grad_norm': '315.88', 'counters/examples': 6272, 'counters/updates': 49}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 52628.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 52628.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 52628.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52628.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52532.2 MB / 81920.0 MB
Train stats after 6400 examples: {'logps_train/chosen': '-301.22', 'loss/train': '301.22', 'examples_per_second': '14.829', 'grad_norm': '308.48', 'counters/examples': 6400, 'counters/updates': 50}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 52628.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52532.2 MB / 81920.0 MB
Train stats after 6528 examples: {'logps_train/chosen': '-351.72', 'loss/train': '351.72', 'examples_per_second': '17.515', 'grad_norm': '366.82', 'counters/examples': 6528, 'counters/updates': 51}
Train stats after 6656 examples: {'logps_train/chosen': '-258.32', 'loss/train': '258.32', 'examples_per_second': '20.538', 'grad_norm': '276.84', 'counters/examples': 6656, 'counters/updates': 52}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 6784 examples: {'logps_train/chosen': '-345.86', 'loss/train': '345.86', 'examples_per_second': '18.546', 'grad_norm': '300.1', 'counters/examples': 6784, 'counters/updates': 53}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 6912 examples: {'logps_train/chosen': '-318.89', 'loss/train': '318.89', 'examples_per_second': '16.406', 'grad_norm': '292.25', 'counters/examples': 6912, 'counters/updates': 54}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7040 examples: {'logps_train/chosen': '-310.71', 'loss/train': '310.71', 'examples_per_second': '19.302', 'grad_norm': '301.83', 'counters/examples': 7040, 'counters/updates': 55}
Train stats after 7168 examples: {'logps_train/chosen': '-278.18', 'loss/train': '278.18', 'examples_per_second': '17.466', 'grad_norm': '310.57', 'counters/examples': 7168, 'counters/updates': 56}
[GPU Monitor] GPU 0: 利用率 30% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 6% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7296 examples: {'logps_train/chosen': '-299.5', 'loss/train': '299.5', 'examples_per_second': '17.848', 'grad_norm': '362.15', 'counters/examples': 7296, 'counters/updates': 57}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7424 examples: {'logps_train/chosen': '-251.08', 'loss/train': '251.08', 'examples_per_second': '21.344', 'grad_norm': '301.09', 'counters/examples': 7424, 'counters/updates': 58}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7552 examples: {'logps_train/chosen': '-274.89', 'loss/train': '274.89', 'examples_per_second': '18.501', 'grad_norm': '329.14', 'counters/examples': 7552, 'counters/updates': 59}
Train stats after 7680 examples: {'logps_train/chosen': '-294.25', 'loss/train': '294.25', 'examples_per_second': '20.968', 'grad_norm': '299.64', 'counters/examples': 7680, 'counters/updates': 60}
[GPU Monitor] GPU 0: 利用率 69% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 83% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 93% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 86% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7808 examples: {'logps_train/chosen': '-295.9', 'loss/train': '295.9', 'examples_per_second': '26.965', 'grad_norm': '350.72', 'counters/examples': 7808, 'counters/updates': 61}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 52540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 95% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 52634.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 96% | 内存使用 52636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 52540.2 MB / 81920.0 MB
Train stats after 7936 examples: {'logps_train/chosen': '-336.07', 'loss/train': '336.07', 'examples_per_second': '25.777', 'grad_norm': '460.63', 'counters/examples': 7936, 'counters/updates': 62}
Train stats after 8064 examples: {'logps_train/chosen': '-314.18', 'loss/train': '314.18', 'examples_per_second': '16.217', 'grad_norm': '313.01', 'counters/examples': 8064, 'counters/updates': 63}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 59656.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 73% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 79% | 内存使用 59656.2 MB / 81920.0 MB
Train stats after 8192 examples: {'logps_train/chosen': '-284.56', 'loss/train': '284.56', 'examples_per_second': '16.74', 'grad_norm': '297.62', 'counters/examples': 8192, 'counters/updates': 64}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59656.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 95% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 59656.2 MB / 81920.0 MB
Train stats after 8320 examples: {'logps_train/chosen': '-293.53', 'loss/train': '293.53', 'examples_per_second': '28.533', 'grad_norm': '311.24', 'counters/examples': 8320, 'counters/updates': 65}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 8448 examples: {'logps_train/chosen': '-258.79', 'loss/train': '258.79', 'examples_per_second': '18.806', 'grad_norm': '271.78', 'counters/examples': 8448, 'counters/updates': 66}
Train stats after 8576 examples: {'logps_train/chosen': '-311.39', 'loss/train': '311.39', 'examples_per_second': '16.348', 'grad_norm': '318.2', 'counters/examples': 8576, 'counters/updates': 67}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 8704 examples: {'logps_train/chosen': '-261.97', 'loss/train': '261.97', 'examples_per_second': '23.663', 'grad_norm': '269.94', 'counters/examples': 8704, 'counters/updates': 68}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 8832 examples: {'logps_train/chosen': '-318.25', 'loss/train': '318.25', 'examples_per_second': '19.291', 'grad_norm': '289.91', 'counters/examples': 8832, 'counters/updates': 69}
Train stats after 8960 examples: {'logps_train/chosen': '-315.75', 'loss/train': '315.75', 'examples_per_second': '18.281', 'grad_norm': '290.32', 'counters/examples': 8960, 'counters/updates': 70}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 82% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9088 examples: {'logps_train/chosen': '-297.1', 'loss/train': '297.1', 'examples_per_second': '20.623', 'grad_norm': '274.57', 'counters/examples': 9088, 'counters/updates': 71}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9216 examples: {'logps_train/chosen': '-320.74', 'loss/train': '320.74', 'examples_per_second': '27.042', 'grad_norm': '312.02', 'counters/examples': 9216, 'counters/updates': 72}
Train stats after 9344 examples: {'logps_train/chosen': '-279.42', 'loss/train': '279.42', 'examples_per_second': '20.778', 'grad_norm': '435.96', 'counters/examples': 9344, 'counters/updates': 73}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9472 examples: {'logps_train/chosen': '-293.35', 'loss/train': '293.35', 'examples_per_second': '18.402', 'grad_norm': '309.46', 'counters/examples': 9472, 'counters/updates': 74}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9600 examples: {'logps_train/chosen': '-250.2', 'loss/train': '250.2', 'examples_per_second': '20.127', 'grad_norm': '327.65', 'counters/examples': 9600, 'counters/updates': 75}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9728 examples: {'logps_train/chosen': '-268.14', 'loss/train': '268.14', 'examples_per_second': '20.717', 'grad_norm': '325.94', 'counters/examples': 9728, 'counters/updates': 76}
Train stats after 9856 examples: {'logps_train/chosen': '-309.97', 'loss/train': '309.97', 'examples_per_second': '19.139', 'grad_norm': '318.92', 'counters/examples': 9856, 'counters/updates': 77}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 9984 examples: {'logps_train/chosen': '-254.21', 'loss/train': '254.21', 'examples_per_second': '20.112', 'grad_norm': '312.64', 'counters/examples': 9984, 'counters/updates': 78}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 95% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 10112 examples: {'logps_train/chosen': '-300.91', 'loss/train': '300.91', 'examples_per_second': '21.246', 'grad_norm': '310.45', 'counters/examples': 10112, 'counters/updates': 79}
Train stats after 10240 examples: {'logps_train/chosen': '-306.29', 'loss/train': '306.29', 'examples_per_second': '23.537', 'grad_norm': '318.76', 'counters/examples': 10240, 'counters/updates': 80}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 96% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 10368 examples: {'logps_train/chosen': '-334.22', 'loss/train': '334.22', 'examples_per_second': '23.752', 'grad_norm': '353.83', 'counters/examples': 10368, 'counters/updates': 81}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 10496 examples: {'logps_train/chosen': '-274.61', 'loss/train': '274.61', 'examples_per_second': '22.285', 'grad_norm': '337.59', 'counters/examples': 10496, 'counters/updates': 82}
Train stats after 10624 examples: {'logps_train/chosen': '-291.05', 'loss/train': '291.05', 'examples_per_second': '21.515', 'grad_norm': '324.8', 'counters/examples': 10624, 'counters/updates': 83}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 10752 examples: {'logps_train/chosen': '-317.12', 'loss/train': '317.12', 'examples_per_second': '16.574', 'grad_norm': '313.61', 'counters/examples': 10752, 'counters/updates': 84}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 10880 examples: {'logps_train/chosen': '-276.64', 'loss/train': '276.64', 'examples_per_second': '20.07', 'grad_norm': '272.16', 'counters/examples': 10880, 'counters/updates': 85}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11008 examples: {'logps_train/chosen': '-258.82', 'loss/train': '258.82', 'examples_per_second': '22.122', 'grad_norm': '261.04', 'counters/examples': 11008, 'counters/updates': 86}
Train stats after 11136 examples: {'logps_train/chosen': '-301.17', 'loss/train': '301.17', 'examples_per_second': '20.847', 'grad_norm': '283.83', 'counters/examples': 11136, 'counters/updates': 87}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11264 examples: {'logps_train/chosen': '-283.54', 'loss/train': '283.54', 'examples_per_second': '20.904', 'grad_norm': '277.46', 'counters/examples': 11264, 'counters/updates': 88}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11392 examples: {'logps_train/chosen': '-233.78', 'loss/train': '233.78', 'examples_per_second': '20.087', 'grad_norm': '270.66', 'counters/examples': 11392, 'counters/updates': 89}
Train stats after 11520 examples: {'logps_train/chosen': '-334.25', 'loss/train': '334.25', 'examples_per_second': '17.721', 'grad_norm': '305.53', 'counters/examples': 11520, 'counters/updates': 90}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11648 examples: {'logps_train/chosen': '-255.66', 'loss/train': '255.66', 'examples_per_second': '18.901', 'grad_norm': '288.48', 'counters/examples': 11648, 'counters/updates': 91}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11776 examples: {'logps_train/chosen': '-297.14', 'loss/train': '297.14', 'examples_per_second': '17.422', 'grad_norm': '295.31', 'counters/examples': 11776, 'counters/updates': 92}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 11904 examples: {'logps_train/chosen': '-320.47', 'loss/train': '320.47', 'examples_per_second': '20.325', 'grad_norm': '298.39', 'counters/examples': 11904, 'counters/updates': 93}
Train stats after 12032 examples: {'logps_train/chosen': '-289.58', 'loss/train': '289.58', 'examples_per_second': '18.455', 'grad_norm': '291.4', 'counters/examples': 12032, 'counters/updates': 94}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 12160 examples: {'logps_train/chosen': '-279.07', 'loss/train': '279.07', 'examples_per_second': '20.441', 'grad_norm': '297.3', 'counters/examples': 12160, 'counters/updates': 95}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 95% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 12288 examples: {'logps_train/chosen': '-307.55', 'loss/train': '307.55', 'examples_per_second': '23.502', 'grad_norm': '304.86', 'counters/examples': 12288, 'counters/updates': 96}
Train stats after 12416 examples: {'logps_train/chosen': '-302.77', 'loss/train': '302.77', 'examples_per_second': '27.174', 'grad_norm': '297.1', 'counters/examples': 12416, 'counters/updates': 97}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 12544 examples: {'logps_train/chosen': '-300.44', 'loss/train': '300.44', 'examples_per_second': '19.851', 'grad_norm': '326.89', 'counters/examples': 12544, 'counters/updates': 98}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 12672 examples: {'logps_train/chosen': '-336.81', 'loss/train': '336.81', 'examples_per_second': '21.933', 'grad_norm': '307.21', 'counters/examples': 12672, 'counters/updates': 99}
Train stats after 12800 examples: {'logps_train/chosen': '-308.37', 'loss/train': '308.37', 'examples_per_second': '19.89', 'grad_norm': '293.85', 'counters/examples': 12800, 'counters/updates': 100}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 12928 examples: {'logps_train/chosen': '-343.92', 'loss/train': '343.92', 'examples_per_second': '18.66', 'grad_norm': '308.03', 'counters/examples': 12928, 'counters/updates': 101}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13056 examples: {'logps_train/chosen': '-300.61', 'loss/train': '300.61', 'examples_per_second': '17.625', 'grad_norm': '300.9', 'counters/examples': 13056, 'counters/updates': 102}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13184 examples: {'logps_train/chosen': '-287.89', 'loss/train': '287.89', 'examples_per_second': '17.428', 'grad_norm': '349.73', 'counters/examples': 13184, 'counters/updates': 103}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 95% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13312 examples: {'logps_train/chosen': '-292.62', 'loss/train': '292.62', 'examples_per_second': '20.49', 'grad_norm': '315.77', 'counters/examples': 13312, 'counters/updates': 104}
Train stats after 13440 examples: {'logps_train/chosen': '-277.16', 'loss/train': '277.16', 'examples_per_second': '20.235', 'grad_norm': '343.28', 'counters/examples': 13440, 'counters/updates': 105}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13568 examples: {'logps_train/chosen': '-243.64', 'loss/train': '243.64', 'examples_per_second': '19.214', 'grad_norm': '262.19', 'counters/examples': 13568, 'counters/updates': 106}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13696 examples: {'logps_train/chosen': '-348.85', 'loss/train': '348.85', 'examples_per_second': '20.247', 'grad_norm': '280.39', 'counters/examples': 13696, 'counters/updates': 107}
Train stats after 13824 examples: {'logps_train/chosen': '-296.14', 'loss/train': '296.14', 'examples_per_second': '22.287', 'grad_norm': '262.01', 'counters/examples': 13824, 'counters/updates': 108}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 13952 examples: {'logps_train/chosen': '-268', 'loss/train': '268', 'examples_per_second': '20.997', 'grad_norm': '258.04', 'counters/examples': 13952, 'counters/updates': 109}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14080 examples: {'logps_train/chosen': '-286.78', 'loss/train': '286.78', 'examples_per_second': '22.993', 'grad_norm': '277.45', 'counters/examples': 14080, 'counters/updates': 110}
Train stats after 14208 examples: {'logps_train/chosen': '-299.41', 'loss/train': '299.41', 'examples_per_second': '18.575', 'grad_norm': '280.94', 'counters/examples': 14208, 'counters/updates': 111}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14336 examples: {'logps_train/chosen': '-318.43', 'loss/train': '318.43', 'examples_per_second': '19.149', 'grad_norm': '307.49', 'counters/examples': 14336, 'counters/updates': 112}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 95% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14464 examples: {'logps_train/chosen': '-280.82', 'loss/train': '280.82', 'examples_per_second': '23.052', 'grad_norm': '306.04', 'counters/examples': 14464, 'counters/updates': 113}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14592 examples: {'logps_train/chosen': '-251.53', 'loss/train': '251.53', 'examples_per_second': '16.069', 'grad_norm': '240.02', 'counters/examples': 14592, 'counters/updates': 114}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14720 examples: {'logps_train/chosen': '-317.14', 'loss/train': '317.14', 'examples_per_second': '16.637', 'grad_norm': '294.17', 'counters/examples': 14720, 'counters/updates': 115}
Train stats after 14848 examples: {'logps_train/chosen': '-264.99', 'loss/train': '264.99', 'examples_per_second': '26.884', 'grad_norm': '262.23', 'counters/examples': 14848, 'counters/updates': 116}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 59838.2 MB / 81920.0 MB
Train stats after 14976 examples: {'logps_train/chosen': '-243.06', 'loss/train': '243.06', 'examples_per_second': '17.35', 'grad_norm': '252.34', 'counters/examples': 14976, 'counters/updates': 117}
Finished generating 1 epochs on train split
[GPU Monitor] GPU 0: 利用率 11% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-4b-test-1-sft/LATEST/policy.pt ...
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 0: 利用率 10% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-4b-test-1-sft/LATEST/optimizer.pt ...
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 59838.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59752.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59778.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59934.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59838.2 MB / 81920.0 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-4b-test-1-sft/LATEST/scheduler.pt ...
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2592:2592 [3] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO ncclCommInitRank comm 0x164ab720 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO NVLS multicast support is not available on dev 3
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO comm 0x164ab720 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2592:3508 [3] NCCL INFO ncclCommInitRank comm 0x164ab720 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x321f6cee1b4de3ff - Init COMPLETE
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2364:2364 [1] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO ncclCommInitRank comm 0x15308cd0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 13000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO NVLS multicast support is not available on dev 1
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO comm 0x15308cd0 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2364:3511 [1] NCCL INFO ncclCommInitRank comm 0x15308cd0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 13000 commId 0x321f6cee1b4de3ff - Init COMPLETE
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2897:2897 [5] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO ncclCommInitRank comm 0x146ae9a0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId ca000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO NVLS multicast support is not available on dev 5
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO comm 0x146ae9a0 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2897:3509 [5] NCCL INFO ncclCommInitRank comm 0x146ae9a0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId ca000 commId 0x321f6cee1b4de3ff - Init COMPLETE
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:3200:3200 [7] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO ncclCommInitRank comm 0x160ea790 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId e4000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO NVLS multicast support is not available on dev 7
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO comm 0x160ea790 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:3200:3507 [7] NCCL INFO ncclCommInitRank comm 0x160ea790 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId e4000 commId 0x321f6cee1b4de3ff - Init COMPLETE
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO cudaDriverVersion 12040
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO Bootstrap : Using eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003347-ql84n-worker-0:2744:2744 [4] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO ncclCommInitRank comm 0x154f8f00 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId c5000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO NVLS multicast support is not available on dev 4
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO comm 0x154f8f00 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2744:3510 [4] NCCL INFO ncclCommInitRank comm 0x154f8f00 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId c5000 commId 0x321f6cee1b4de3ff - Init COMPLETE
t-20250803003347-ql84n-worker-0:2277:2277 [0] NCCL INFO Comm config Blocking set to 1
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO P2P plugin IBext
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.218<0>
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Using non-device net plugin version 0
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Using network IBext
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO ncclCommInitRank comm 0x132ca110 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId d000 commId 0x321f6cee1b4de3ff - Init START
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO NVLS multicast support is not available on dev 0
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO comm 0x132ca110 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 00/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 01/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 02/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 03/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 04/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 05/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 06/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 07/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 08/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 09/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 10/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 11/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 12/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 13/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 14/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 15/16 :    0   1   2   3   4   5   6   7
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO P2P Chunksize set to 524288
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Connected all rings
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO Connected all trees
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003347-ql84n-worker-0:2277:3506 [0] NCCL INFO ncclCommInitRank comm 0x132ca110 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId d000 commId 0x321f6cee1b4de3ff - Init COMPLETE
