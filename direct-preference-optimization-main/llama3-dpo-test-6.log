WARNING: eval_every must be divisible by batch_size
Setting eval_every to 19968
no FSDP port specified; using open port for FSDP: 34549
seed: 0
exp_name: llama3-8b-test-6-dpo
batch_size: 128
eval_batch_size: 16
debug: false
fsdp_port: 34549
datasets:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/dataset/ultra-3000.jsonl
wandb:
  enabled: true
  entity: null
  project: direct-preference-optimization
local_dirs:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-dpo
lr: 5.0e-06
gradient_accumulation_steps: 16
max_grad_norm: 10.0
max_length: 2048
max_prompt_length: 512
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 19968
minimum_log_interval_secs: 1.0
model:
  name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Llama3-8B-Base
  tokenizer_name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Llama3-8B-Base
  archive: /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-sft/LATEST/policy.pt
  block_name: LlamaDecoderLayer
  reference_name_or_path: null
  policy_dtype: bfloat16
  fsdp_policy_mp: bfloat16
  reference_dtype: bfloat16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to t-20250801153619-kt7nx-worker-0:/fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-dpo
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 83.44it/s]
building reference model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 85.39it/s]
loading pre-trained weights at step 2944 from /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-sft/LATEST/policy.pt with metrics {}
loaded pre-trained weights
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 1048576 from 1048576
[rank1]:[W801 07:50:25.713995027 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W801 07:50:29.257511424 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W801 07:50:33.311957167 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W801 07:50:37.222233628 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W801 07:50:41.433254575 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W801 07:50:46.872625785 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W801 07:50:50.556061806 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W801 07:50:50.640149864 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer /fs-computility/llmit_d/shared/zhangchi/wjc/Llama3-8B-Base
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 3.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 9077.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 9077.0 MB / 81251.2 MB
Loaded train data iterator
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 9077.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 9077.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 9077.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 25% | 内存使用 6997.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 1% | 内存使用 417.0 MB / 81251.2 MB
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy with FSDP...
Sharding reference model with FSDP...
Loaded model on rank 0
Using RMSprop optimizer
=== Running evaluation after 0 train examples ===
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:01<00:21,  1.45s/it]Computing eval metrics:  12%|█▎        | 2/16 [00:02<00:13,  1.03it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:03<00:12,  1.03it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:03<00:10,  1.16it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:04<00:08,  1.33it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:05<00:07,  1.35it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:05<00:06,  1.37it/s]Computing eval metrics:  50%|█████     | 8/16 [00:06<00:06,  1.28it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:07<00:05,  1.32it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:08<00:04,  1.26it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:09<00:04,  1.24it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:09<00:03,  1.23it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:10<00:02,  1.36it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:11<00:01,  1.20it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:12<00:00,  1.20it/s]Computing eval metrics: 100%|██████████| 16/16 [00:13<00:00,  1.25it/s]Computing eval metrics: 100%|██████████| 16/16 [00:13<00:00,  1.23it/s]
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
4 initializing distributed
Creating trainer on process 4 with world size 8
1 initializing distributed
Creating trainer on process 1 with world size 8
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:5990:5990 [4] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO ncclCommInitRank comm 0x10fa9670 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b3000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO NVLS multicast support is not available on dev 4
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO comm 0x10fa9670 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:5990:6760 [4] NCCL INFO ncclCommInitRank comm 0x10fa9670 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b3000 commId 0x4b50e2e308de4874 - Init COMPLETE
2 initializing distributed
Creating trainer on process 2 with world size 8
7 initializing distributed
Creating trainer on process 7 with world size 8
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:5686:5686 [2] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO ncclCommInitRank comm 0x11c14a80 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 69000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO NVLS multicast support is not available on dev 2
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO comm 0x11c14a80 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:5686:6759 [2] NCCL INFO ncclCommInitRank comm 0x11c14a80 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 69000 commId 0x4b50e2e308de4874 - Init COMPLETE
6 initializing distributed
Creating trainer on process 6 with world size 8
5 initializing distributed
Creating trainer on process 5 with world size 8
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:6295:6295 [6] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO ncclCommInitRank comm 0x105a6530 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d5000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO NVLS multicast support is not available on dev 6
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO comm 0x105a6530 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:6295:6763 [6] NCCL INFO ncclCommInitRank comm 0x105a6530 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d5000 commId 0x4b50e2e308de4874 - Init COMPLETE
3 initializing distributed
Creating trainer on process 3 with world size 8
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 11729.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 11825.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 11729.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 44967.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 45063.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 44967.0 MB / 81251.2 MB
Eval after 0 examples: {'rewards_eval/chosen': '0.00046629', 'rewards_eval/rejected': '-0.0015973', 'rewards_eval/accuracies': '0.53125', 'rewards_eval/margins': '0.0020636', 'logps_eval/rejected': '-220.77', 'logps_eval/chosen': '-274.88', 'loss/eval': '0.69285'}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 59079.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59079.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 59079.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 59175.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 59079.0 MB / 81251.2 MB
Train stats after 128 examples: {'rewards_train/chosen': '0.0020294', 'rewards_train/rejected': '-0.010922', 'rewards_train/accuracies': '0.57031', 'rewards_train/margins': '0.012952', 'logps_train/rejected': '-237.32', 'logps_train/chosen': '-278.31', 'loss/train': '0.68733', 'examples_per_second': '7.0895', 'grad_norm': '41.75', 'counters/examples': 128, 'counters/updates': 1}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 64763.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 64763.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 64763.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 64859.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 64763.0 MB / 81251.2 MB
Train stats after 256 examples: {'rewards_train/chosen': '-0.0023162', 'rewards_train/rejected': '-0.0055387', 'rewards_train/accuracies': '0.50781', 'rewards_train/margins': '0.0032225', 'logps_train/rejected': '-204.26', 'logps_train/chosen': '-271.48', 'loss/train': '0.69235', 'examples_per_second': '6.5048', 'grad_norm': '37', 'counters/examples': 256, 'counters/updates': 2}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 384 examples: {'rewards_train/chosen': '0.0012874', 'rewards_train/rejected': '-0.00068395', 'rewards_train/accuracies': '0.50781', 'rewards_train/margins': '0.0019713', 'logps_train/rejected': '-175.96', 'logps_train/chosen': '-249.74', 'loss/train': '0.6929', 'examples_per_second': '4.9285', 'grad_norm': '38.5', 'counters/examples': 384, 'counters/updates': 3}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 512 examples: {'rewards_train/chosen': '0.011557', 'rewards_train/rejected': '-0.0035843', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015142', 'logps_train/rejected': '-207.89', 'logps_train/chosen': '-287.86', 'loss/train': '0.68641', 'examples_per_second': '7.689', 'grad_norm': '44.25', 'counters/examples': 512, 'counters/updates': 4}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 640 examples: {'rewards_train/chosen': '-0.00055058', 'rewards_train/rejected': '-0.0040274', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.0034768', 'logps_train/rejected': '-216.6', 'logps_train/chosen': '-251.94', 'loss/train': '0.69201', 'examples_per_second': '6.9861', 'grad_norm': '37.25', 'counters/examples': 640, 'counters/updates': 5}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 768 examples: {'rewards_train/chosen': '0.012428', 'rewards_train/rejected': '-0.021801', 'rewards_train/accuracies': '0.71094', 'rewards_train/margins': '0.034229', 'logps_train/rejected': '-211.72', 'logps_train/chosen': '-273.81', 'loss/train': '0.67697', 'examples_per_second': '8.0988', 'grad_norm': '38.75', 'counters/examples': 768, 'counters/updates': 6}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 896 examples: {'rewards_train/chosen': '0.00099534', 'rewards_train/rejected': '-0.029983', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.030978', 'logps_train/rejected': '-203.75', 'logps_train/chosen': '-268.54', 'loss/train': '0.67866', 'examples_per_second': '6.1035', 'grad_norm': '40', 'counters/examples': 896, 'counters/updates': 7}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1024 examples: {'rewards_train/chosen': '0.013555', 'rewards_train/rejected': '-0.037472', 'rewards_train/accuracies': '0.66406', 'rewards_train/margins': '0.051027', 'logps_train/rejected': '-191.43', 'logps_train/chosen': '-218.71', 'loss/train': '0.66935', 'examples_per_second': '7.7056', 'grad_norm': '33', 'counters/examples': 1024, 'counters/updates': 8}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1152 examples: {'rewards_train/chosen': '0.0306', 'rewards_train/rejected': '-0.04566', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.07626', 'logps_train/rejected': '-235.82', 'logps_train/chosen': '-278.86', 'loss/train': '0.65736', 'examples_per_second': '6.8291', 'grad_norm': '38.75', 'counters/examples': 1152, 'counters/updates': 9}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1280 examples: {'rewards_train/chosen': '0.017785', 'rewards_train/rejected': '-0.067569', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085354', 'logps_train/rejected': '-200.55', 'logps_train/chosen': '-239.54', 'loss/train': '0.65368', 'examples_per_second': '5.6687', 'grad_norm': '35.25', 'counters/examples': 1280, 'counters/updates': 10}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1408 examples: {'rewards_train/chosen': '0.032272', 'rewards_train/rejected': '-0.087318', 'rewards_train/accuracies': '0.77344', 'rewards_train/margins': '0.11959', 'logps_train/rejected': '-205.8', 'logps_train/chosen': '-265.43', 'loss/train': '0.63888', 'examples_per_second': '6.8567', 'grad_norm': '35.75', 'counters/examples': 1408, 'counters/updates': 11}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1536 examples: {'rewards_train/chosen': '0.036869', 'rewards_train/rejected': '-0.096514', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-202.08', 'logps_train/chosen': '-260.88', 'loss/train': '0.63129', 'examples_per_second': '6.7487', 'grad_norm': '36.25', 'counters/examples': 1536, 'counters/updates': 12}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1664 examples: {'rewards_train/chosen': '0.055121', 'rewards_train/rejected': '-0.097852', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15297', 'logps_train/rejected': '-211.44', 'logps_train/chosen': '-257.32', 'loss/train': '0.6246', 'examples_per_second': '6.0727', 'grad_norm': '34.25', 'counters/examples': 1664, 'counters/updates': 13}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1792 examples: {'rewards_train/chosen': '0.028506', 'rewards_train/rejected': '-0.1476', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.17611', 'logps_train/rejected': '-177.43', 'logps_train/chosen': '-231.62', 'loss/train': '0.61427', 'examples_per_second': '6.5772', 'grad_norm': '33', 'counters/examples': 1792, 'counters/updates': 14}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 1920 examples: {'rewards_train/chosen': '0.073907', 'rewards_train/rejected': '-0.15973', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.23363', 'logps_train/rejected': '-207.63', 'logps_train/chosen': '-247.66', 'loss/train': '0.59383', 'examples_per_second': '6.9615', 'grad_norm': '32.5', 'counters/examples': 1920, 'counters/updates': 15}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2048 examples: {'rewards_train/chosen': '0.091418', 'rewards_train/rejected': '-0.26888', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '0.36029', 'logps_train/rejected': '-289.35', 'logps_train/chosen': '-288.39', 'loss/train': '0.54058', 'examples_per_second': '6.0888', 'grad_norm': '36.25', 'counters/examples': 2048, 'counters/updates': 16}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2176 examples: {'rewards_train/chosen': '0.080122', 'rewards_train/rejected': '-0.31872', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '0.39884', 'logps_train/rejected': '-257.72', 'logps_train/chosen': '-262.55', 'loss/train': '0.53364', 'examples_per_second': '5.3756', 'grad_norm': '32.5', 'counters/examples': 2176, 'counters/updates': 17}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2304 examples: {'rewards_train/chosen': '0.070975', 'rewards_train/rejected': '-0.28992', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.3609', 'logps_train/rejected': '-197.45', 'logps_train/chosen': '-233.58', 'loss/train': '0.54688', 'examples_per_second': '6.8696', 'grad_norm': '29.875', 'counters/examples': 2304, 'counters/updates': 18}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2432 examples: {'rewards_train/chosen': '0.072548', 'rewards_train/rejected': '-0.41191', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.48446', 'logps_train/rejected': '-214', 'logps_train/chosen': '-291.7', 'loss/train': '0.50968', 'examples_per_second': '5.921', 'grad_norm': '29.5', 'counters/examples': 2432, 'counters/updates': 19}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2560 examples: {'rewards_train/chosen': '0.071927', 'rewards_train/rejected': '-0.53294', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.60486', 'logps_train/rejected': '-223.26', 'logps_train/chosen': '-273.84', 'loss/train': '0.47696', 'examples_per_second': '5.6889', 'grad_norm': '28.5', 'counters/examples': 2560, 'counters/updates': 20}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2688 examples: {'rewards_train/chosen': '0.027268', 'rewards_train/rejected': '-0.63807', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '0.66534', 'logps_train/rejected': '-224.68', 'logps_train/chosen': '-286.25', 'loss/train': '0.46726', 'examples_per_second': '6.2692', 'grad_norm': '26.625', 'counters/examples': 2688, 'counters/updates': 21}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2816 examples: {'rewards_train/chosen': '-0.015177', 'rewards_train/rejected': '-0.74216', 'rewards_train/accuracies': '0.89062', 'rewards_train/margins': '0.72699', 'logps_train/rejected': '-213.09', 'logps_train/chosen': '-308.16', 'loss/train': '0.44789', 'examples_per_second': '5.1779', 'grad_norm': '28', 'counters/examples': 2816, 'counters/updates': 22}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 74229.0 MB / 81251.2 MB
Train stats after 2944 examples: {'rewards_train/chosen': '-0.0075033', 'rewards_train/rejected': '-0.78443', 'rewards_train/accuracies': '0.89844', 'rewards_train/margins': '0.77693', 'logps_train/rejected': '-214.85', 'logps_train/chosen': '-277.58', 'loss/train': '0.44079', 'examples_per_second': '5.6747', 'grad_norm': '25.75', 'counters/examples': 2944, 'counters/updates': 23}
Finished generating 1 epochs on train split
[GPU Monitor] GPU 0: 利用率 10% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-dpo/LATEST/policy.pt ...
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
[GPU Monitor] GPU 0: 利用率 9% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-dpo/LATEST/optimizer.pt ...
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 74227.0 MB / 81251.2 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 74323.0 MB / 81251.2 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 74325.0 MB / 81251.2 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 74229.0 MB / 81251.2 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/llama3-8b-test-6-dpo/LATEST/scheduler.pt ...
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:5608:5608 [1] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO ncclCommInitRank comm 0x11769a70 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 52000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO NVLS multicast support is not available on dev 1
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO comm 0x11769a70 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:5608:6764 [1] NCCL INFO ncclCommInitRank comm 0x11769a70 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 52000 commId 0x4b50e2e308de4874 - Init COMPLETE
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:6446:6446 [7] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO ncclCommInitRank comm 0x1226ffe0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d8000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO NVLS multicast support is not available on dev 7
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO comm 0x1226ffe0 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:6446:6758 [7] NCCL INFO ncclCommInitRank comm 0x1226ffe0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d8000 commId 0x4b50e2e308de4874 - Init COMPLETE
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:6141:6141 [5] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO ncclCommInitRank comm 0x11525260 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b7000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO NVLS multicast support is not available on dev 5
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO comm 0x11525260 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:6141:6762 [5] NCCL INFO ncclCommInitRank comm 0x11525260 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b7000 commId 0x4b50e2e308de4874 - Init COMPLETE
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO cudaDriverVersion 12040
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO Bootstrap : Using eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250801153619-kt7nx-worker-0:5839:5839 [3] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO ncclCommInitRank comm 0x1063d7e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO NVLS multicast support is not available on dev 3
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO comm 0x1063d7e0 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:5839:6761 [3] NCCL INFO ncclCommInitRank comm 0x1063d7e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x4b50e2e308de4874 - Init COMPLETE
t-20250801153619-kt7nx-worker-0:5522:5522 [0] NCCL INFO Comm config Blocking set to 1
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO P2P plugin IBext
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.27.144<0>
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Using non-device net plugin version 0
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Using network IBext
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO ncclCommInitRank comm 0x10505f70 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x4b50e2e308de4874 - Init START
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO NVLS multicast support is not available on dev 0
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO comm 0x10505f70 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 00/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 01/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 02/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 03/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 04/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 05/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 06/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 07/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 08/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 09/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 10/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 11/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 12/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 13/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 14/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 15/16 :    0   1   2   3   4   5   6   7
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO P2P Chunksize set to 524288
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Connected all rings
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO Connected all trees
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250801153619-kt7nx-worker-0:5522:6757 [0] NCCL INFO ncclCommInitRank comm 0x10505f70 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x4b50e2e308de4874 - Init COMPLETE
