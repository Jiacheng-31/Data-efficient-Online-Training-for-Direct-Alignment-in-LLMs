WARNING: eval_every must be divisible by batch_size
Setting eval_every to 19968
no FSDP port specified; using open port for FSDP: 52375
seed: 0
exp_name: qwen3-1.7b-test-1-dpo
batch_size: 128
eval_batch_size: 16
debug: false
fsdp_port: 52375
datasets:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/dataset/ultra-train-dataset.jsonl
wandb:
  enabled: true
  entity: null
  project: direct-preference-optimization
local_dirs:
- /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-dpo
lr: 5.0e-06
gradient_accumulation_steps: 16
max_grad_norm: 10.0
max_length: 2048
max_prompt_length: 512
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 19968
minimum_log_interval_secs: 1.0
model:
  name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-1.7B
  tokenizer_name_or_path: /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-1.7B
  archive: /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-sft/LATEST/policy.pt
  block_name: Qwen3DecoderLayer
  reference_name_or_path: null
  policy_dtype: bfloat16
  fsdp_policy_mp: bfloat16
  reference_dtype: bfloat16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to t-20250803003407-lqxt8-worker-0:/fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-dpo
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.61it/s]
building reference model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.69it/s]
loading pre-trained weights at step 14976 from /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-sft/LATEST/policy.pt with metrics {}
loaded pre-trained weights
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 1048576 from 1048576
[rank1]:[W802 16:43:57.040422332 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W802 16:44:01.002704617 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W802 16:44:05.999722175 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W802 16:44:10.227086225 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W802 16:44:14.535941756 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W802 16:44:18.668658536 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W802 16:44:22.965238508 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W802 16:44:22.973705922 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer /fs-computility/llmit_d/shared/zhangchi/wjc/Qwen3-1.7B
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 696.8 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 3576.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 1114.2 MB / 81920.0 MB
Loaded train data iterator
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy with FSDP...
Sharding reference model with FSDP...
Loaded model on rank 0
Using RMSprop optimizer
=== Running evaluation after 0 train examples ===
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:12,  1.16it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:01<00:06,  2.12it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:01<00:04,  2.66it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.02it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:03,  3.27it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:02<00:02,  3.49it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:02<00:02,  3.20it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.23it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:03<00:02,  3.34it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:03<00:01,  3.70it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:03<00:01,  3.56it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.14it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:04<00:01,  2.99it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:04<00:00,  2.93it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:05<00:00,  2.73it/s]Computing eval metrics: 100%|██████████| 16/16 [00:05<00:00,  2.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:05<00:00,  2.94it/s]
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/DPO/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
5 initializing distributed
Creating trainer on process 5 with world size 8
7 initializing distributed
Creating trainer on process 7 with world size 8
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6755:6755 [5] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO ncclCommInitRank comm 0x159ba3e0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId ca000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO NVLS multicast support is not available on dev 5
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO comm 0x159ba3e0 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6755:7354 [5] NCCL INFO ncclCommInitRank comm 0x159ba3e0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId ca000 commId 0x5903d3914e1e9de6 - Init COMPLETE
4 initializing distributed
Creating trainer on process 4 with world size 8
2 initializing distributed
Creating trainer on process 2 with world size 8
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6604:6604 [4] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO ncclCommInitRank comm 0x15d67f20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId c5000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO NVLS multicast support is not available on dev 4
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO comm 0x15d67f20 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6604:7358 [4] NCCL INFO ncclCommInitRank comm 0x15d67f20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId c5000 commId 0x5903d3914e1e9de6 - Init COMPLETE
3 initializing distributed
Creating trainer on process 3 with world size 8
6 initializing distributed
Creating trainer on process 6 with world size 8
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6451:6451 [3] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO ncclCommInitRank comm 0x15cedca0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO NVLS multicast support is not available on dev 3
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO comm 0x15cedca0 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6451:7352 [3] NCCL INFO ncclCommInitRank comm 0x15cedca0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x5903d3914e1e9de6 - Init COMPLETE
1 initializing distributed
Creating trainer on process 1 with world size 8
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 49766.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 49862.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 49766.2 MB / 81920.0 MB
Eval after 0 examples: {'rewards_eval/chosen': '0.0044816', 'rewards_eval/rejected': '-0.0087138', 'rewards_eval/accuracies': '0.58203', 'rewards_eval/margins': '0.013195', 'logps_eval/rejected': '-230.84', 'logps_eval/chosen': '-343.84', 'loss/eval': '0.6874'}
Train stats after 128 examples: {'rewards_train/chosen': '-0.0005738', 'rewards_train/rejected': '-0.0039864', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0034126', 'logps_train/rejected': '-245.99', 'logps_train/chosen': '-323.99', 'loss/train': '0.69229', 'examples_per_second': '18.225', 'grad_norm': '28.375', 'counters/examples': 128, 'counters/updates': 1}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51540.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51636.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51540.2 MB / 81920.0 MB
Train stats after 256 examples: {'rewards_train/chosen': '0.0047817', 'rewards_train/rejected': '-0.0055414', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010323', 'logps_train/rejected': '-215.63', 'logps_train/chosen': '-363.79', 'loss/train': '0.68879', 'examples_per_second': '15.83', 'grad_norm': '26.875', 'counters/examples': 256, 'counters/updates': 2}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 384 examples: {'rewards_train/chosen': '0.0032021', 'rewards_train/rejected': '-0.0077095', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010912', 'logps_train/rejected': '-191.95', 'logps_train/chosen': '-326.14', 'loss/train': '0.68889', 'examples_per_second': '14.047', 'grad_norm': '26.5', 'counters/examples': 384, 'counters/updates': 3}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 512 examples: {'rewards_train/chosen': '0.0078827', 'rewards_train/rejected': '0.0062211', 'rewards_train/accuracies': '0.44531', 'rewards_train/margins': '0.0016616', 'logps_train/rejected': '-223.66', 'logps_train/chosen': '-329.49', 'loss/train': '0.69315', 'examples_per_second': '15.52', 'grad_norm': '27.125', 'counters/examples': 512, 'counters/updates': 4}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 640 examples: {'rewards_train/chosen': '0.0049823', 'rewards_train/rejected': '-0.0011953', 'rewards_train/accuracies': '0.55469', 'rewards_train/margins': '0.0061775', 'logps_train/rejected': '-228.87', 'logps_train/chosen': '-307.3', 'loss/train': '0.69087', 'examples_per_second': '16.923', 'grad_norm': '25.25', 'counters/examples': 640, 'counters/updates': 5}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 768 examples: {'rewards_train/chosen': '-0.0010292', 'rewards_train/rejected': '-0.0010659', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '3.6659e-05', 'logps_train/rejected': '-198.74', 'logps_train/chosen': '-271.67', 'loss/train': '0.69392', 'examples_per_second': '13.711', 'grad_norm': '24.625', 'counters/examples': 768, 'counters/updates': 6}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 896 examples: {'rewards_train/chosen': '0.002422', 'rewards_train/rejected': '-0.0079517', 'rewards_train/accuracies': '0.55469', 'rewards_train/margins': '0.010374', 'logps_train/rejected': '-245.69', 'logps_train/chosen': '-331.1', 'loss/train': '0.68901', 'examples_per_second': '14.053', 'grad_norm': '26', 'counters/examples': 896, 'counters/updates': 7}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51542.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 1024 examples: {'rewards_train/chosen': '0.0010069', 'rewards_train/rejected': '0.0023993', 'rewards_train/accuracies': '0.47656', 'rewards_train/margins': '-0.0013924', 'logps_train/rejected': '-214.9', 'logps_train/chosen': '-314.78', 'loss/train': '0.695', 'examples_per_second': '14.773', 'grad_norm': '27.5', 'counters/examples': 1024, 'counters/updates': 8}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 1152 examples: {'rewards_train/chosen': '-0.0010278', 'rewards_train/rejected': '-0.0051124', 'rewards_train/accuracies': '0.48438', 'rewards_train/margins': '0.0040847', 'logps_train/rejected': '-231.09', 'logps_train/chosen': '-287.03', 'loss/train': '0.69186', 'examples_per_second': '12.714', 'grad_norm': '24.5', 'counters/examples': 1152, 'counters/updates': 9}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 1280 examples: {'rewards_train/chosen': '-0.0028914', 'rewards_train/rejected': '-0.016951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01406', 'logps_train/rejected': '-254.45', 'logps_train/chosen': '-316.23', 'loss/train': '0.68729', 'examples_per_second': '19.425', 'grad_norm': '29.25', 'counters/examples': 1280, 'counters/updates': 10}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51638.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51542.2 MB / 81920.0 MB
Train stats after 1408 examples: {'rewards_train/chosen': '0.0071237', 'rewards_train/rejected': '-0.011531', 'rewards_train/accuracies': '0.55469', 'rewards_train/margins': '0.018655', 'logps_train/rejected': '-269.57', 'logps_train/chosen': '-346.5', 'loss/train': '0.68497', 'examples_per_second': '15.88', 'grad_norm': '28.5', 'counters/examples': 1408, 'counters/updates': 11}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 1536 examples: {'rewards_train/chosen': '0.012534', 'rewards_train/rejected': '-0.0135', 'rewards_train/accuracies': '0.64844', 'rewards_train/margins': '0.026034', 'logps_train/rejected': '-280.42', 'logps_train/chosen': '-340.68', 'loss/train': '0.68115', 'examples_per_second': '12.964', 'grad_norm': '27.625', 'counters/examples': 1536, 'counters/updates': 12}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 1664 examples: {'rewards_train/chosen': '0.0019553', 'rewards_train/rejected': '-0.026071', 'rewards_train/accuracies': '0.67969', 'rewards_train/margins': '0.028026', 'logps_train/rejected': '-224.64', 'logps_train/chosen': '-320.53', 'loss/train': '0.68012', 'examples_per_second': '17.837', 'grad_norm': '27.25', 'counters/examples': 1664, 'counters/updates': 13}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 1792 examples: {'rewards_train/chosen': '0.0073', 'rewards_train/rejected': '-0.018904', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.026204', 'logps_train/rejected': '-181.98', 'logps_train/chosen': '-342.55', 'loss/train': '0.68106', 'examples_per_second': '14.043', 'grad_norm': '28.125', 'counters/examples': 1792, 'counters/updates': 14}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 1920 examples: {'rewards_train/chosen': '0.027464', 'rewards_train/rejected': '-0.023951', 'rewards_train/accuracies': '0.71094', 'rewards_train/margins': '0.051414', 'logps_train/rejected': '-224.22', 'logps_train/chosen': '-331.19', 'loss/train': '0.66914', 'examples_per_second': '13.96', 'grad_norm': '31.125', 'counters/examples': 1920, 'counters/updates': 15}
Train stats after 2048 examples: {'rewards_train/chosen': '0.0041904', 'rewards_train/rejected': '-0.029627', 'rewards_train/accuracies': '0.66406', 'rewards_train/margins': '0.033817', 'logps_train/rejected': '-257.06', 'logps_train/chosen': '-300.58', 'loss/train': '0.67745', 'examples_per_second': '17.348', 'grad_norm': '26.125', 'counters/examples': 2048, 'counters/updates': 16}
[GPU Monitor] GPU 0: 利用率 48% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 91% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 36% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 77% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 50% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 31% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2176 examples: {'rewards_train/chosen': '0.019646', 'rewards_train/rejected': '-0.025678', 'rewards_train/accuracies': '0.64844', 'rewards_train/margins': '0.045323', 'logps_train/rejected': '-249.76', 'logps_train/chosen': '-369.81', 'loss/train': '0.67218', 'examples_per_second': '13.628', 'grad_norm': '26.75', 'counters/examples': 2176, 'counters/updates': 17}
Train stats after 2304 examples: {'rewards_train/chosen': '0.034881', 'rewards_train/rejected': '-0.027081', 'rewards_train/accuracies': '0.71094', 'rewards_train/margins': '0.061962', 'logps_train/rejected': '-245.34', 'logps_train/chosen': '-371.44', 'loss/train': '0.66405', 'examples_per_second': '15.832', 'grad_norm': '33.75', 'counters/examples': 2304, 'counters/updates': 18}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2432 examples: {'rewards_train/chosen': '0.008618', 'rewards_train/rejected': '-0.037489', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.046107', 'logps_train/rejected': '-235.55', 'logps_train/chosen': '-327.67', 'loss/train': '0.67154', 'examples_per_second': '17.498', 'grad_norm': '28', 'counters/examples': 2432, 'counters/updates': 19}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2560 examples: {'rewards_train/chosen': '0.013044', 'rewards_train/rejected': '-0.041626', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.05467', 'logps_train/rejected': '-227.8', 'logps_train/chosen': '-317.95', 'loss/train': '0.66763', 'examples_per_second': '13.519', 'grad_norm': '25.375', 'counters/examples': 2560, 'counters/updates': 20}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2688 examples: {'rewards_train/chosen': '0.017579', 'rewards_train/rejected': '-0.040879', 'rewards_train/accuracies': '0.67969', 'rewards_train/margins': '0.058459', 'logps_train/rejected': '-227.46', 'logps_train/chosen': '-284.08', 'loss/train': '0.66577', 'examples_per_second': '16.674', 'grad_norm': '26.75', 'counters/examples': 2688, 'counters/updates': 21}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2816 examples: {'rewards_train/chosen': '0.0058315', 'rewards_train/rejected': '-0.047202', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.053034', 'logps_train/rejected': '-232.9', 'logps_train/chosen': '-322.46', 'loss/train': '0.66832', 'examples_per_second': '13.222', 'grad_norm': '26.75', 'counters/examples': 2816, 'counters/updates': 22}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 2944 examples: {'rewards_train/chosen': '0.02859', 'rewards_train/rejected': '-0.047309', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.075899', 'logps_train/rejected': '-253.83', 'logps_train/chosen': '-384.29', 'loss/train': '0.65793', 'examples_per_second': '16.504', 'grad_norm': '27.5', 'counters/examples': 2944, 'counters/updates': 23}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3072 examples: {'rewards_train/chosen': '0.0091827', 'rewards_train/rejected': '-0.059951', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069133', 'logps_train/rejected': '-226.02', 'logps_train/chosen': '-334.91', 'loss/train': '0.661', 'examples_per_second': '12.803', 'grad_norm': '25.75', 'counters/examples': 3072, 'counters/updates': 24}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3200 examples: {'rewards_train/chosen': '0.010535', 'rewards_train/rejected': '-0.063888', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.074422', 'logps_train/rejected': '-187.44', 'logps_train/chosen': '-262.3', 'loss/train': '0.65868', 'examples_per_second': '15.707', 'grad_norm': '24', 'counters/examples': 3200, 'counters/updates': 25}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3328 examples: {'rewards_train/chosen': '0.019002', 'rewards_train/rejected': '-0.067932', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.086934', 'logps_train/rejected': '-249.09', 'logps_train/chosen': '-311.81', 'loss/train': '0.65286', 'examples_per_second': '18.496', 'grad_norm': '29', 'counters/examples': 3328, 'counters/updates': 26}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3456 examples: {'rewards_train/chosen': '0.021866', 'rewards_train/rejected': '-0.089915', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.11178', 'logps_train/rejected': '-226.81', 'logps_train/chosen': '-335.52', 'loss/train': '0.64204', 'examples_per_second': '16.156', 'grad_norm': '26.25', 'counters/examples': 3456, 'counters/updates': 27}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3584 examples: {'rewards_train/chosen': '0.029039', 'rewards_train/rejected': '-0.082152', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.11119', 'logps_train/rejected': '-261.09', 'logps_train/chosen': '-333.36', 'loss/train': '0.64155', 'examples_per_second': '15.732', 'grad_norm': '26.5', 'counters/examples': 3584, 'counters/updates': 28}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3712 examples: {'rewards_train/chosen': '0.018477', 'rewards_train/rejected': '-0.088704', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.10718', 'logps_train/rejected': '-223.53', 'logps_train/chosen': '-324.82', 'loss/train': '0.6443', 'examples_per_second': '16.661', 'grad_norm': '27.125', 'counters/examples': 3712, 'counters/updates': 29}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3840 examples: {'rewards_train/chosen': '0.01387', 'rewards_train/rejected': '-0.10051', 'rewards_train/accuracies': '0.72656', 'rewards_train/margins': '0.11438', 'logps_train/rejected': '-214.1', 'logps_train/chosen': '-323.44', 'loss/train': '0.64183', 'examples_per_second': '13.288', 'grad_norm': '25.125', 'counters/examples': 3840, 'counters/updates': 30}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 3968 examples: {'rewards_train/chosen': '0.029074', 'rewards_train/rejected': '-0.11514', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.14421', 'logps_train/rejected': '-269.73', 'logps_train/chosen': '-349.56', 'loss/train': '0.62771', 'examples_per_second': '15.7', 'grad_norm': '26.625', 'counters/examples': 3968, 'counters/updates': 31}
Train stats after 4096 examples: {'rewards_train/chosen': '0.01625', 'rewards_train/rejected': '-0.11763', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13388', 'logps_train/rejected': '-254.81', 'logps_train/chosen': '-288.45', 'loss/train': '0.63212', 'examples_per_second': '17.061', 'grad_norm': '26', 'counters/examples': 4096, 'counters/updates': 32}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4224 examples: {'rewards_train/chosen': '0.016328', 'rewards_train/rejected': '-0.14339', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.15972', 'logps_train/rejected': '-252.72', 'logps_train/chosen': '-319.95', 'loss/train': '0.62066', 'examples_per_second': '18.067', 'grad_norm': '25.625', 'counters/examples': 4224, 'counters/updates': 33}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4352 examples: {'rewards_train/chosen': '0.022695', 'rewards_train/rejected': '-0.13679', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.15948', 'logps_train/rejected': '-226.72', 'logps_train/chosen': '-311.3', 'loss/train': '0.62155', 'examples_per_second': '16.571', 'grad_norm': '24.875', 'counters/examples': 4352, 'counters/updates': 34}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4480 examples: {'rewards_train/chosen': '0.043404', 'rewards_train/rejected': '-0.16611', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '0.20951', 'logps_train/rejected': '-209.69', 'logps_train/chosen': '-322.78', 'loss/train': '0.60034', 'examples_per_second': '16.336', 'grad_norm': '26.25', 'counters/examples': 4480, 'counters/updates': 35}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4608 examples: {'rewards_train/chosen': '0.018806', 'rewards_train/rejected': '-0.15691', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.17571', 'logps_train/rejected': '-240.59', 'logps_train/chosen': '-324.21', 'loss/train': '0.61553', 'examples_per_second': '20.317', 'grad_norm': '24.5', 'counters/examples': 4608, 'counters/updates': 36}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4736 examples: {'rewards_train/chosen': '0.027259', 'rewards_train/rejected': '-0.14787', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.17513', 'logps_train/rejected': '-217.22', 'logps_train/chosen': '-292.98', 'loss/train': '0.61426', 'examples_per_second': '15.79', 'grad_norm': '23.875', 'counters/examples': 4736, 'counters/updates': 37}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4864 examples: {'rewards_train/chosen': '0.014011', 'rewards_train/rejected': '-0.1821', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.19611', 'logps_train/rejected': '-213.1', 'logps_train/chosen': '-328.64', 'loss/train': '0.60815', 'examples_per_second': '12.596', 'grad_norm': '23.5', 'counters/examples': 4864, 'counters/updates': 38}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 4992 examples: {'rewards_train/chosen': '0.013849', 'rewards_train/rejected': '-0.18619', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.20004', 'logps_train/rejected': '-237.71', 'logps_train/chosen': '-306.35', 'loss/train': '0.60528', 'examples_per_second': '15.693', 'grad_norm': '24.25', 'counters/examples': 4992, 'counters/updates': 39}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5120 examples: {'rewards_train/chosen': '0.011578', 'rewards_train/rejected': '-0.21223', 'rewards_train/accuracies': '0.75781', 'rewards_train/margins': '0.22381', 'logps_train/rejected': '-272.76', 'logps_train/chosen': '-341.27', 'loss/train': '0.59919', 'examples_per_second': '13.387', 'grad_norm': '23.125', 'counters/examples': 5120, 'counters/updates': 40}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5248 examples: {'rewards_train/chosen': '0.036837', 'rewards_train/rejected': '-0.202', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '0.23883', 'logps_train/rejected': '-226.71', 'logps_train/chosen': '-361.62', 'loss/train': '0.59058', 'examples_per_second': '17.231', 'grad_norm': '23.75', 'counters/examples': 5248, 'counters/updates': 41}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5376 examples: {'rewards_train/chosen': '0.025149', 'rewards_train/rejected': '-0.22737', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.25252', 'logps_train/rejected': '-226', 'logps_train/chosen': '-357.93', 'loss/train': '0.58873', 'examples_per_second': '12.972', 'grad_norm': '23.25', 'counters/examples': 5376, 'counters/updates': 42}
Train stats after 5504 examples: {'rewards_train/chosen': '0.024026', 'rewards_train/rejected': '-0.25188', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27591', 'logps_train/rejected': '-249.88', 'logps_train/chosen': '-324.83', 'loss/train': '0.58031', 'examples_per_second': '16.647', 'grad_norm': '22.25', 'counters/examples': 5504, 'counters/updates': 43}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 89% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 93% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5632 examples: {'rewards_train/chosen': '0.030706', 'rewards_train/rejected': '-0.26082', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '0.29153', 'logps_train/rejected': '-220.01', 'logps_train/chosen': '-282.37', 'loss/train': '0.56798', 'examples_per_second': '16.422', 'grad_norm': '23.625', 'counters/examples': 5632, 'counters/updates': 44}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 38% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 76% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 76% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 82% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 27% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5760 examples: {'rewards_train/chosen': '-0.005743', 'rewards_train/rejected': '-0.33565', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.32991', 'logps_train/rejected': '-258.83', 'logps_train/chosen': '-355.78', 'loss/train': '0.55668', 'examples_per_second': '16.953', 'grad_norm': '24.625', 'counters/examples': 5760, 'counters/updates': 45}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 5888 examples: {'rewards_train/chosen': '-0.0049003', 'rewards_train/rejected': '-0.30614', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '0.30124', 'logps_train/rejected': '-244.58', 'logps_train/chosen': '-317.07', 'loss/train': '0.56914', 'examples_per_second': '16.629', 'grad_norm': '23.25', 'counters/examples': 5888, 'counters/updates': 46}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 95% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 95% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6016 examples: {'rewards_train/chosen': '-0.044627', 'rewards_train/rejected': '-0.29625', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.25162', 'logps_train/rejected': '-206.65', 'logps_train/chosen': '-287.61', 'loss/train': '0.58848', 'examples_per_second': '20.563', 'grad_norm': '21.875', 'counters/examples': 6016, 'counters/updates': 47}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6144 examples: {'rewards_train/chosen': '0.013095', 'rewards_train/rejected': '-0.33723', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.35032', 'logps_train/rejected': '-255.77', 'logps_train/chosen': '-380.15', 'loss/train': '0.55254', 'examples_per_second': '14.937', 'grad_norm': '24.625', 'counters/examples': 6144, 'counters/updates': 48}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6272 examples: {'rewards_train/chosen': '-0.023461', 'rewards_train/rejected': '-0.40315', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.37969', 'logps_train/rejected': '-213.9', 'logps_train/chosen': '-285.94', 'loss/train': '0.53797', 'examples_per_second': '12.746', 'grad_norm': '22.625', 'counters/examples': 6272, 'counters/updates': 49}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6400 examples: {'rewards_train/chosen': '0.00089552', 'rewards_train/rejected': '-0.43425', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.43515', 'logps_train/rejected': '-215.22', 'logps_train/chosen': '-310.65', 'loss/train': '0.52738', 'examples_per_second': '12.41', 'grad_norm': '21.25', 'counters/examples': 6400, 'counters/updates': 50}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 96% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6528 examples: {'rewards_train/chosen': '-0.058748', 'rewards_train/rejected': '-0.38047', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.32173', 'logps_train/rejected': '-255.17', 'logps_train/chosen': '-362.6', 'loss/train': '0.56654', 'examples_per_second': '13.177', 'grad_norm': '22', 'counters/examples': 6528, 'counters/updates': 51}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6656 examples: {'rewards_train/chosen': '-0.037069', 'rewards_train/rejected': '-0.4479', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.41083', 'logps_train/rejected': '-242.1', 'logps_train/chosen': '-270.99', 'loss/train': '0.53306', 'examples_per_second': '13.414', 'grad_norm': '22.125', 'counters/examples': 6656, 'counters/updates': 52}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6784 examples: {'rewards_train/chosen': '-0.007796', 'rewards_train/rejected': '-0.58959', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.5818', 'logps_train/rejected': '-234.5', 'logps_train/chosen': '-360.26', 'loss/train': '0.48148', 'examples_per_second': '14.364', 'grad_norm': '22.5', 'counters/examples': 6784, 'counters/updates': 53}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 6912 examples: {'rewards_train/chosen': '-0.11003', 'rewards_train/rejected': '-0.58135', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.47132', 'logps_train/rejected': '-230.64', 'logps_train/chosen': '-332.22', 'loss/train': '0.52521', 'examples_per_second': '12.938', 'grad_norm': '19.625', 'counters/examples': 6912, 'counters/updates': 54}
[GPU Monitor] GPU 0: 利用率 96% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7040 examples: {'rewards_train/chosen': '-0.034397', 'rewards_train/rejected': '-0.53433', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.49993', 'logps_train/rejected': '-220.55', 'logps_train/chosen': '-326.86', 'loss/train': '0.50576', 'examples_per_second': '14.842', 'grad_norm': '20.375', 'counters/examples': 7040, 'counters/updates': 55}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7168 examples: {'rewards_train/chosen': '-0.20984', 'rewards_train/rejected': '-0.66495', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45511', 'logps_train/rejected': '-291.24', 'logps_train/chosen': '-289.06', 'loss/train': '0.53839', 'examples_per_second': '13.323', 'grad_norm': '20', 'counters/examples': 7168, 'counters/updates': 56}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7296 examples: {'rewards_train/chosen': '-0.15346', 'rewards_train/rejected': '-0.61724', 'rewards_train/accuracies': '0.77344', 'rewards_train/margins': '0.46378', 'logps_train/rejected': '-239.96', 'logps_train/chosen': '-313.2', 'loss/train': '0.52396', 'examples_per_second': '13.735', 'grad_norm': '20.375', 'counters/examples': 7296, 'counters/updates': 57}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7424 examples: {'rewards_train/chosen': '-0.11283', 'rewards_train/rejected': '-0.60634', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.49351', 'logps_train/rejected': '-237.04', 'logps_train/chosen': '-266.26', 'loss/train': '0.51386', 'examples_per_second': '16.332', 'grad_norm': '19.25', 'counters/examples': 7424, 'counters/updates': 58}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7552 examples: {'rewards_train/chosen': '-0.17708', 'rewards_train/rejected': '-0.64226', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46518', 'logps_train/rejected': '-211.32', 'logps_train/chosen': '-283.48', 'loss/train': '0.52508', 'examples_per_second': '14.12', 'grad_norm': '19.875', 'counters/examples': 7552, 'counters/updates': 59}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7680 examples: {'rewards_train/chosen': '-0.15749', 'rewards_train/rejected': '-0.63474', 'rewards_train/accuracies': '0.78906', 'rewards_train/margins': '0.47725', 'logps_train/rejected': '-251.59', 'logps_train/chosen': '-311.08', 'loss/train': '0.51876', 'examples_per_second': '15.872', 'grad_norm': '19.125', 'counters/examples': 7680, 'counters/updates': 60}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7808 examples: {'rewards_train/chosen': '-0.15943', 'rewards_train/rejected': '-0.82731', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '0.66788', 'logps_train/rejected': '-254.24', 'logps_train/chosen': '-315.05', 'loss/train': '0.45308', 'examples_per_second': '18.795', 'grad_norm': '20.875', 'counters/examples': 7808, 'counters/updates': 61}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 7936 examples: {'rewards_train/chosen': '-0.23645', 'rewards_train/rejected': '-0.75963', 'rewards_train/accuracies': '0.80469', 'rewards_train/margins': '0.52318', 'logps_train/rejected': '-250.64', 'logps_train/chosen': '-355.8', 'loss/train': '0.50432', 'examples_per_second': '18.05', 'grad_norm': '21.25', 'counters/examples': 7936, 'counters/updates': 62}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
Train stats after 8064 examples: {'rewards_train/chosen': '-0.25508', 'rewards_train/rejected': '-0.97118', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '0.7161', 'logps_train/rejected': '-268.54', 'logps_train/chosen': '-330.52', 'loss/train': '0.45888', 'examples_per_second': '12.457', 'grad_norm': '18.75', 'counters/examples': 8064, 'counters/updates': 63}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 8192 examples: {'rewards_train/chosen': '-0.31373', 'rewards_train/rejected': '-0.91149', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '0.59777', 'logps_train/rejected': '-224.34', 'logps_train/chosen': '-301.15', 'loss/train': '0.49392', 'examples_per_second': '12.913', 'grad_norm': '19.125', 'counters/examples': 8192, 'counters/updates': 64}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 95% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 8320 examples: {'rewards_train/chosen': '-0.29161', 'rewards_train/rejected': '-0.97793', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.68633', 'logps_train/rejected': '-243.13', 'logps_train/chosen': '-309.88', 'loss/train': '0.47377', 'examples_per_second': '22.298', 'grad_norm': '18.5', 'counters/examples': 8320, 'counters/updates': 65}
Train stats after 8448 examples: {'rewards_train/chosen': '-0.23885', 'rewards_train/rejected': '-0.93013', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '0.69128', 'logps_train/rejected': '-218.27', 'logps_train/chosen': '-276.01', 'loss/train': '0.45157', 'examples_per_second': '14.903', 'grad_norm': '19.25', 'counters/examples': 8448, 'counters/updates': 66}
[GPU Monitor] GPU 0: 利用率 8% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 24% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 45% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 27% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 2% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 56% | 内存使用 51546.2 MB / 81920.0 MB
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 8576 examples: {'rewards_train/chosen': '-0.2516', 'rewards_train/rejected': '-1.0177', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.76606', 'logps_train/rejected': '-253.53', 'logps_train/chosen': '-325.74', 'loss/train': '0.4443', 'examples_per_second': '12.939', 'grad_norm': '19', 'counters/examples': 8576, 'counters/updates': 67}
Train stats after 8704 examples: {'rewards_train/chosen': '-0.25868', 'rewards_train/rejected': '-0.90145', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.64277', 'logps_train/rejected': '-220.13', 'logps_train/chosen': '-279', 'loss/train': '0.48667', 'examples_per_second': '18.171', 'grad_norm': '17.375', 'counters/examples': 8704, 'counters/updates': 68}
[GPU Monitor] GPU 0: 利用率 57% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 38% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 62% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 77% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 44% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 8832 examples: {'rewards_train/chosen': '-0.27562', 'rewards_train/rejected': '-0.93346', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '0.65784', 'logps_train/rejected': '-259.54', 'logps_train/chosen': '-333.61', 'loss/train': '0.473', 'examples_per_second': '14.985', 'grad_norm': '19.5', 'counters/examples': 8832, 'counters/updates': 69}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 8960 examples: {'rewards_train/chosen': '-0.24229', 'rewards_train/rejected': '-0.98262', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.74034', 'logps_train/rejected': '-216.44', 'logps_train/chosen': '-335.35', 'loss/train': '0.44604', 'examples_per_second': '13.668', 'grad_norm': '17.875', 'counters/examples': 8960, 'counters/updates': 70}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 87% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 85% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9088 examples: {'rewards_train/chosen': '-0.4008', 'rewards_train/rejected': '-1.1238', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '0.72304', 'logps_train/rejected': '-225.21', 'logps_train/chosen': '-310.43', 'loss/train': '0.4638', 'examples_per_second': '16.056', 'grad_norm': '16.875', 'counters/examples': 9088, 'counters/updates': 71}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9216 examples: {'rewards_train/chosen': '-0.26566', 'rewards_train/rejected': '-1.1143', 'rewards_train/accuracies': '0.86719', 'rewards_train/margins': '0.84861', 'logps_train/rejected': '-257.38', 'logps_train/chosen': '-337.19', 'loss/train': '0.41763', 'examples_per_second': '16.553', 'grad_norm': '18.375', 'counters/examples': 9216, 'counters/updates': 72}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9344 examples: {'rewards_train/chosen': '-0.40844', 'rewards_train/rejected': '-1.217', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '0.80852', 'logps_train/rejected': '-236.64', 'logps_train/chosen': '-296.57', 'loss/train': '0.44135', 'examples_per_second': '14.732', 'grad_norm': '17.625', 'counters/examples': 9344, 'counters/updates': 73}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9472 examples: {'rewards_train/chosen': '-0.49168', 'rewards_train/rejected': '-1.2114', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71973', 'logps_train/rejected': '-237.77', 'logps_train/chosen': '-310.86', 'loss/train': '0.48359', 'examples_per_second': '14.003', 'grad_norm': '18.75', 'counters/examples': 9472, 'counters/updates': 74}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9600 examples: {'rewards_train/chosen': '-0.41194', 'rewards_train/rejected': '-1.0849', 'rewards_train/accuracies': '0.78906', 'rewards_train/margins': '0.67295', 'logps_train/rejected': '-240.39', 'logps_train/chosen': '-266.22', 'loss/train': '0.47505', 'examples_per_second': '15.752', 'grad_norm': '18.5', 'counters/examples': 9600, 'counters/updates': 75}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9728 examples: {'rewards_train/chosen': '-0.40037', 'rewards_train/rejected': '-1.1194', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.71901', 'logps_train/rejected': '-255.48', 'logps_train/chosen': '-281.58', 'loss/train': '0.4719', 'examples_per_second': '16.041', 'grad_norm': '19.125', 'counters/examples': 9728, 'counters/updates': 76}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9856 examples: {'rewards_train/chosen': '-0.31813', 'rewards_train/rejected': '-1.1812', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '0.8631', 'logps_train/rejected': '-260.23', 'logps_train/chosen': '-326.53', 'loss/train': '0.41559', 'examples_per_second': '14.671', 'grad_norm': '17.625', 'counters/examples': 9856, 'counters/updates': 77}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 9984 examples: {'rewards_train/chosen': '-0.35403', 'rewards_train/rejected': '-1.088', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.73396', 'logps_train/rejected': '-186.65', 'logps_train/chosen': '-268.11', 'loss/train': '0.46803', 'examples_per_second': '13.525', 'grad_norm': '17.125', 'counters/examples': 9984, 'counters/updates': 78}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10112 examples: {'rewards_train/chosen': '-0.36655', 'rewards_train/rejected': '-1.2148', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.84822', 'logps_train/rejected': '-230.01', 'logps_train/chosen': '-321.05', 'loss/train': '0.44306', 'examples_per_second': '16.501', 'grad_norm': '18', 'counters/examples': 10112, 'counters/updates': 79}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10240 examples: {'rewards_train/chosen': '-0.29973', 'rewards_train/rejected': '-1.3966', 'rewards_train/accuracies': '0.89844', 'rewards_train/margins': '1.0969', 'logps_train/rejected': '-254.93', 'logps_train/chosen': '-323.78', 'loss/train': '0.35949', 'examples_per_second': '17.906', 'grad_norm': '18.125', 'counters/examples': 10240, 'counters/updates': 80}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10368 examples: {'rewards_train/chosen': '-0.54161', 'rewards_train/rejected': '-1.4458', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.90423', 'logps_train/rejected': '-280.4', 'logps_train/chosen': '-355.52', 'loss/train': '0.44489', 'examples_per_second': '15.859', 'grad_norm': '19.25', 'counters/examples': 10368, 'counters/updates': 81}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10496 examples: {'rewards_train/chosen': '-0.49658', 'rewards_train/rejected': '-1.4788', 'rewards_train/accuracies': '0.89844', 'rewards_train/margins': '0.98219', 'logps_train/rejected': '-248.98', 'logps_train/chosen': '-297.88', 'loss/train': '0.39603', 'examples_per_second': '12.673', 'grad_norm': '16.625', 'counters/examples': 10496, 'counters/updates': 82}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10624 examples: {'rewards_train/chosen': '-0.43967', 'rewards_train/rejected': '-1.4038', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '0.96415', 'logps_train/rejected': '-245.21', 'logps_train/chosen': '-314.79', 'loss/train': '0.39802', 'examples_per_second': '16.523', 'grad_norm': '17.25', 'counters/examples': 10624, 'counters/updates': 83}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10752 examples: {'rewards_train/chosen': '-0.5537', 'rewards_train/rejected': '-1.5825', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.0287', 'logps_train/rejected': '-255.67', 'logps_train/chosen': '-337.06', 'loss/train': '0.40055', 'examples_per_second': '12.789', 'grad_norm': '16.875', 'counters/examples': 10752, 'counters/updates': 84}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 10880 examples: {'rewards_train/chosen': '-0.42748', 'rewards_train/rejected': '-1.4886', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '1.0612', 'logps_train/rejected': '-241.69', 'logps_train/chosen': '-295.24', 'loss/train': '0.40783', 'examples_per_second': '15.551', 'grad_norm': '15.5', 'counters/examples': 10880, 'counters/updates': 85}
Train stats after 11008 examples: {'rewards_train/chosen': '-0.51064', 'rewards_train/rejected': '-1.5686', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.0579', 'logps_train/rejected': '-247.92', 'logps_train/chosen': '-275.47', 'loss/train': '0.38938', 'examples_per_second': '17.253', 'grad_norm': '17.5', 'counters/examples': 11008, 'counters/updates': 86}
[GPU Monitor] GPU 0: 利用率 95% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 74% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 44% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 58% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 55% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11136 examples: {'rewards_train/chosen': '-0.44164', 'rewards_train/rejected': '-1.5884', 'rewards_train/accuracies': '0.83594', 'rewards_train/margins': '1.1468', 'logps_train/rejected': '-229.03', 'logps_train/chosen': '-320.04', 'loss/train': '0.37723', 'examples_per_second': '15.059', 'grad_norm': '16.25', 'counters/examples': 11136, 'counters/updates': 87}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11264 examples: {'rewards_train/chosen': '-0.53766', 'rewards_train/rejected': '-1.5948', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '1.0571', 'logps_train/rejected': '-227.38', 'logps_train/chosen': '-298.86', 'loss/train': '0.38555', 'examples_per_second': '16.146', 'grad_norm': '16.75', 'counters/examples': 11264, 'counters/updates': 88}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11392 examples: {'rewards_train/chosen': '-0.55481', 'rewards_train/rejected': '-1.7622', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '1.2074', 'logps_train/rejected': '-220.37', 'logps_train/chosen': '-245.39', 'loss/train': '0.38687', 'examples_per_second': '15.762', 'grad_norm': '13.812', 'counters/examples': 11392, 'counters/updates': 89}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11520 examples: {'rewards_train/chosen': '-0.61795', 'rewards_train/rejected': '-1.7331', 'rewards_train/accuracies': '0.86719', 'rewards_train/margins': '1.1151', 'logps_train/rejected': '-293.14', 'logps_train/chosen': '-358.64', 'loss/train': '0.38776', 'examples_per_second': '13.72', 'grad_norm': '17.25', 'counters/examples': 11520, 'counters/updates': 90}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11648 examples: {'rewards_train/chosen': '-0.5106', 'rewards_train/rejected': '-1.786', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '1.2754', 'logps_train/rejected': '-260.31', 'logps_train/chosen': '-269.16', 'loss/train': '0.34176', 'examples_per_second': '14.813', 'grad_norm': '15.688', 'counters/examples': 11648, 'counters/updates': 91}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11776 examples: {'rewards_train/chosen': '-0.80401', 'rewards_train/rejected': '-2.062', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '1.258', 'logps_train/rejected': '-260.54', 'logps_train/chosen': '-317.28', 'loss/train': '0.35725', 'examples_per_second': '16.193', 'grad_norm': '17.25', 'counters/examples': 11776, 'counters/updates': 92}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 11904 examples: {'rewards_train/chosen': '-0.78068', 'rewards_train/rejected': '-2.3168', 'rewards_train/accuracies': '0.86719', 'rewards_train/margins': '1.5361', 'logps_train/rejected': '-270.2', 'logps_train/chosen': '-346.68', 'loss/train': '0.30226', 'examples_per_second': '13.41', 'grad_norm': '15.812', 'counters/examples': 11904, 'counters/updates': 93}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12032 examples: {'rewards_train/chosen': '-0.88439', 'rewards_train/rejected': '-2.0522', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '1.1678', 'logps_train/rejected': '-268.7', 'logps_train/chosen': '-309.19', 'loss/train': '0.43147', 'examples_per_second': '14.06', 'grad_norm': '17.375', 'counters/examples': 12032, 'counters/updates': 94}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12160 examples: {'rewards_train/chosen': '-0.6534', 'rewards_train/rejected': '-1.9423', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '1.2889', 'logps_train/rejected': '-278.18', 'logps_train/chosen': '-297.39', 'loss/train': '0.36759', 'examples_per_second': '15.916', 'grad_norm': '15.125', 'counters/examples': 12160, 'counters/updates': 95}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12288 examples: {'rewards_train/chosen': '-0.6349', 'rewards_train/rejected': '-2.0141', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '1.3792', 'logps_train/rejected': '-253.74', 'logps_train/chosen': '-323.9', 'loss/train': '0.33238', 'examples_per_second': '17.336', 'grad_norm': '15.062', 'counters/examples': 12288, 'counters/updates': 96}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12416 examples: {'rewards_train/chosen': '-0.7307', 'rewards_train/rejected': '-2.0108', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '1.2801', 'logps_train/rejected': '-211.53', 'logps_train/chosen': '-323.62', 'loss/train': '0.36032', 'examples_per_second': '18.433', 'grad_norm': '15.562', 'counters/examples': 12416, 'counters/updates': 97}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12544 examples: {'rewards_train/chosen': '-0.78027', 'rewards_train/rejected': '-2.0899', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.3096', 'logps_train/rejected': '-243.29', 'logps_train/chosen': '-327.4', 'loss/train': '0.43441', 'examples_per_second': '15.513', 'grad_norm': '38.75', 'counters/examples': 12544, 'counters/updates': 98}
Train stats after 12672 examples: {'rewards_train/chosen': '-0.58197', 'rewards_train/rejected': '-2.1002', 'rewards_train/accuracies': '0.85938', 'rewards_train/margins': '1.5182', 'logps_train/rejected': '-262.67', 'logps_train/chosen': '-357.93', 'loss/train': '0.32224', 'examples_per_second': '17.021', 'grad_norm': '15.188', 'counters/examples': 12672, 'counters/updates': 99}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 5% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12800 examples: {'rewards_train/chosen': '-0.80918', 'rewards_train/rejected': '-2.1269', 'rewards_train/accuracies': '0.82031', 'rewards_train/margins': '1.3177', 'logps_train/rejected': '-249.82', 'logps_train/chosen': '-332.53', 'loss/train': '0.36743', 'examples_per_second': '15.576', 'grad_norm': '16.875', 'counters/examples': 12800, 'counters/updates': 100}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 12928 examples: {'rewards_train/chosen': '-0.78379', 'rewards_train/rejected': '-2.2883', 'rewards_train/accuracies': '0.88281', 'rewards_train/margins': '1.5045', 'logps_train/rejected': '-280.74', 'logps_train/chosen': '-368.93', 'loss/train': '0.3325', 'examples_per_second': '14.046', 'grad_norm': '17', 'counters/examples': 12928, 'counters/updates': 101}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13056 examples: {'rewards_train/chosen': '-0.73586', 'rewards_train/rejected': '-2.3908', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.6549', 'logps_train/rejected': '-277.88', 'logps_train/chosen': '-319.84', 'loss/train': '0.30157', 'examples_per_second': '13.306', 'grad_norm': '14.625', 'counters/examples': 13056, 'counters/updates': 102}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 81% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 44% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 60% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 88% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13184 examples: {'rewards_train/chosen': '-0.80803', 'rewards_train/rejected': '-1.984', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '1.176', 'logps_train/rejected': '-227.77', 'logps_train/chosen': '-310.19', 'loss/train': '0.42538', 'examples_per_second': '12.988', 'grad_norm': '17.375', 'counters/examples': 13184, 'counters/updates': 103}
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 0% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 0% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 0% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13312 examples: {'rewards_train/chosen': '-0.73909', 'rewards_train/rejected': '-2.2362', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '1.4971', 'logps_train/rejected': '-281.45', 'logps_train/chosen': '-311.49', 'loss/train': '0.34433', 'examples_per_second': '15.874', 'grad_norm': '14.688', 'counters/examples': 13312, 'counters/updates': 104}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13440 examples: {'rewards_train/chosen': '-0.67941', 'rewards_train/rejected': '-2.2173', 'rewards_train/accuracies': '0.9375', 'rewards_train/margins': '1.5378', 'logps_train/rejected': '-233.1', 'logps_train/chosen': '-291.37', 'loss/train': '0.31025', 'examples_per_second': '15.745', 'grad_norm': '14.25', 'counters/examples': 13440, 'counters/updates': 105}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13568 examples: {'rewards_train/chosen': '-0.60236', 'rewards_train/rejected': '-2.2085', 'rewards_train/accuracies': '0.89844', 'rewards_train/margins': '1.6062', 'logps_train/rejected': '-225.32', 'logps_train/chosen': '-259.86', 'loss/train': '0.32476', 'examples_per_second': '14.887', 'grad_norm': '15.312', 'counters/examples': 13568, 'counters/updates': 106}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13696 examples: {'rewards_train/chosen': '-0.81245', 'rewards_train/rejected': '-2.1854', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.3729', 'logps_train/rejected': '-287.11', 'logps_train/chosen': '-370.68', 'loss/train': '0.39092', 'examples_per_second': '15.605', 'grad_norm': '17.375', 'counters/examples': 13696, 'counters/updates': 107}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13824 examples: {'rewards_train/chosen': '-0.7467', 'rewards_train/rejected': '-2.3879', 'rewards_train/accuracies': '0.89844', 'rewards_train/margins': '1.6412', 'logps_train/rejected': '-244.59', 'logps_train/chosen': '-313.62', 'loss/train': '0.29663', 'examples_per_second': '13.228', 'grad_norm': '14.438', 'counters/examples': 13824, 'counters/updates': 108}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 13952 examples: {'rewards_train/chosen': '-0.99141', 'rewards_train/rejected': '-2.4748', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.4834', 'logps_train/rejected': '-226.49', 'logps_train/chosen': '-289.82', 'loss/train': '0.36499', 'examples_per_second': '16.019', 'grad_norm': '17.625', 'counters/examples': 13952, 'counters/updates': 109}
[GPU Monitor] GPU 0: 利用率 100% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 97% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14080 examples: {'rewards_train/chosen': '-0.88576', 'rewards_train/rejected': '-2.384', 'rewards_train/accuracies': '0.85156', 'rewards_train/margins': '1.4982', 'logps_train/rejected': '-259.28', 'logps_train/chosen': '-303.55', 'loss/train': '0.33613', 'examples_per_second': '17.039', 'grad_norm': '15.562', 'counters/examples': 14080, 'counters/updates': 110}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 98% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14208 examples: {'rewards_train/chosen': '-0.91638', 'rewards_train/rejected': '-2.5243', 'rewards_train/accuracies': '0.86719', 'rewards_train/margins': '1.6079', 'logps_train/rejected': '-250.19', 'logps_train/chosen': '-321.94', 'loss/train': '0.33524', 'examples_per_second': '13.97', 'grad_norm': '15.938', 'counters/examples': 14208, 'counters/updates': 111}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 99% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14336 examples: {'rewards_train/chosen': '-0.91686', 'rewards_train/rejected': '-2.576', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.6592', 'logps_train/rejected': '-273.85', 'logps_train/chosen': '-349.48', 'loss/train': '0.30696', 'examples_per_second': '14.95', 'grad_norm': '15.875', 'counters/examples': 14336, 'counters/updates': 112}
[GPU Monitor] GPU 0: 利用率 97% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14464 examples: {'rewards_train/chosen': '-0.89587', 'rewards_train/rejected': '-2.4417', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.5458', 'logps_train/rejected': '-244.76', 'logps_train/chosen': '-302.1', 'loss/train': '0.36733', 'examples_per_second': '17.303', 'grad_norm': '17.75', 'counters/examples': 14464, 'counters/updates': 113}
[GPU Monitor] GPU 0: 利用率 99% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 98% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14592 examples: {'rewards_train/chosen': '-0.96753', 'rewards_train/rejected': '-2.272', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.3044', 'logps_train/rejected': '-233.32', 'logps_train/chosen': '-270.64', 'loss/train': '0.37673', 'examples_per_second': '12.436', 'grad_norm': '17.125', 'counters/examples': 14592, 'counters/updates': 114}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14720 examples: {'rewards_train/chosen': '-0.83257', 'rewards_train/rejected': '-2.5522', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.7196', 'logps_train/rejected': '-262.62', 'logps_train/chosen': '-334.4', 'loss/train': '0.30399', 'examples_per_second': '12.866', 'grad_norm': '15.875', 'counters/examples': 14720, 'counters/updates': 115}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 96% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 96% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 97% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 97% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14848 examples: {'rewards_train/chosen': '-0.82734', 'rewards_train/rejected': '-2.6252', 'rewards_train/accuracies': '0.89062', 'rewards_train/margins': '1.7978', 'logps_train/rejected': '-229.97', 'logps_train/chosen': '-274.41', 'loss/train': '0.31756', 'examples_per_second': '21.243', 'grad_norm': '13.625', 'counters/examples': 14848, 'counters/updates': 116}
[GPU Monitor] GPU 0: 利用率 98% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 99% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 98% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 99% | 内存使用 51546.2 MB / 81920.0 MB
Train stats after 14976 examples: {'rewards_train/chosen': '-1.0143', 'rewards_train/rejected': '-2.7907', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.7763', 'logps_train/rejected': '-229.17', 'logps_train/chosen': '-277.22', 'loss/train': '0.30535', 'examples_per_second': '13.344', 'grad_norm': '15.188', 'counters/examples': 14976, 'counters/updates': 117}
Finished generating 1 epochs on train split
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-dpo/LATEST/policy.pt ...
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-dpo/LATEST/optimizer.pt ...
[GPU Monitor] GPU 0: 利用率 0% | 内存使用 51544.2 MB / 81920.0 MB
[GPU Monitor] GPU 1: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 2: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 3: 利用率 100% | 内存使用 51642.2 MB / 81920.0 MB
[GPU Monitor] GPU 4: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 5: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 6: 利用率 100% | 内存使用 51640.2 MB / 81920.0 MB
[GPU Monitor] GPU 7: 利用率 100% | 内存使用 51546.2 MB / 81920.0 MB
Writing checkpoint to /fs-computility/llmit_d/shared/zhangchi/wjc/DPO/direct-preference-optimization-main/.cache/root/qwen3-1.7b-test-1-dpo/LATEST/scheduler.pt ...
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:7056:7056 [7] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO ncclCommInitRank comm 0x14490930 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId e4000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO NVLS multicast support is not available on dev 7
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO comm 0x14490930 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:7056:7355 [7] NCCL INFO ncclCommInitRank comm 0x14490930 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId e4000 commId 0x5903d3914e1e9de6 - Init COMPLETE
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6298:6298 [2] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO ncclCommInitRank comm 0x15e58850 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO NVLS multicast support is not available on dev 2
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO comm 0x15e58850 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6298:7353 [2] NCCL INFO ncclCommInitRank comm 0x15e58850 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x5903d3914e1e9de6 - Init COMPLETE
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6904:6904 [6] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO ncclCommInitRank comm 0x1676e360 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId e0000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO NVLS multicast support is not available on dev 6
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO comm 0x1676e360 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6904:7356 [6] NCCL INFO ncclCommInitRank comm 0x1676e360 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId e0000 commId 0x5903d3914e1e9de6 - Init COMPLETE
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO cudaDriverVersion 12040
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO Bootstrap : Using eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
t-20250803003407-lqxt8-worker-0:6219:6219 [1] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO ncclCommInitRank comm 0xd025860 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 13000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO NVLS multicast support is not available on dev 1
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO comm 0xd025860 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6219:7357 [1] NCCL INFO ncclCommInitRank comm 0xd025860 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 13000 commId 0x5903d3914e1e9de6 - Init COMPLETE
t-20250803003407-lqxt8-worker-0:6134:6134 [0] NCCL INFO Comm config Blocking set to 1
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO P2P plugin IBext
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.30.37.219<0>
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Using non-device net plugin version 0
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Using network IBext
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO ncclCommInitRank comm 0x10a8e5a0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId d000 commId 0x5903d3914e1e9de6 - Init START
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO NVLS multicast support is not available on dev 0
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO comm 0x10a8e5a0 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 00/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 01/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 02/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 03/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 04/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 05/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 06/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 07/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 08/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 09/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 10/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 11/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 12/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 13/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 14/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 15/16 :    0   1   2   3   4   5   6   7
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO P2P Chunksize set to 524288
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Connected all rings
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO Connected all trees
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
t-20250803003407-lqxt8-worker-0:6134:7350 [0] NCCL INFO ncclCommInitRank comm 0x10a8e5a0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId d000 commId 0x5903d3914e1e9de6 - Init COMPLETE
